{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"712b0ee96b4f4356880e786c9fffad34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2eb612360c0b408ba7a6d5dc1b3a00a1","IPY_MODEL_cf21243fe22749228b065d6178393b22","IPY_MODEL_2c9df83597da404a8924597e3f6d4a64"],"layout":"IPY_MODEL_299188bca98f4649ac5e4ef1b1567926"}},"2eb612360c0b408ba7a6d5dc1b3a00a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2a947b204174eaf82bc1116e5807a33","placeholder":"​","style":"IPY_MODEL_cf00fe9cb8a340a88c5b857b07315f5c","value":"100%"}},"cf21243fe22749228b065d6178393b22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe9ee33360c241619998212768421fac","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f51833b3bae5443a9fa88542e70387ea","value":1}},"2c9df83597da404a8924597e3f6d4a64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88e79be25d2c40c0b625f4a9aefe4e3b","placeholder":"​","style":"IPY_MODEL_61209074c40c49f7b6cce129ae4cc0e9","value":" 1/1 [00:02&lt;00:00,  2.91s/it]"}},"299188bca98f4649ac5e4ef1b1567926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a947b204174eaf82bc1116e5807a33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf00fe9cb8a340a88c5b857b07315f5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe9ee33360c241619998212768421fac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f51833b3bae5443a9fa88542e70387ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88e79be25d2c40c0b625f4a9aefe4e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61209074c40c49f7b6cce129ae4cc0e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86bafea191e1474b86ac6dbf2280c1d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9eb1b43043f24dd691d3aa776f519ccf","IPY_MODEL_77e5135e6b5d4f2085608ffa8995b74c","IPY_MODEL_5eef88c405ba47289272b568c46ab891"],"layout":"IPY_MODEL_c9ed3151f50d46c684a84b9a2fd9d048"}},"9eb1b43043f24dd691d3aa776f519ccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ea9203ed5c744018d20d17ba5befbce","placeholder":"​","style":"IPY_MODEL_33705f67b2aa4feab6f86ba678281090","value":"100%"}},"77e5135e6b5d4f2085608ffa8995b74c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31d442e3ce4f4cd08594feb7685c44d1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed7f2ca35e934f119de7d9ce2f568941","value":1}},"5eef88c405ba47289272b568c46ab891":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2842c8e678f406997801558bd338de9","placeholder":"​","style":"IPY_MODEL_fc8cdf5e81af4851bc36dcee023dd7bb","value":" 1/1 [00:00&lt;00:00, 24.57it/s]"}},"c9ed3151f50d46c684a84b9a2fd9d048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea9203ed5c744018d20d17ba5befbce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33705f67b2aa4feab6f86ba678281090":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31d442e3ce4f4cd08594feb7685c44d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed7f2ca35e934f119de7d9ce2f568941":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2842c8e678f406997801558bd338de9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc8cdf5e81af4851bc36dcee023dd7bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0e45d33cbd84ec6b07ef8acd3fa5fb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc496fbacdef434db21c7a614728fd59","IPY_MODEL_445ecf6ce5b244038938398d2f1cacdd","IPY_MODEL_c854a3b7152f4a43b146fd80904cb9f3"],"layout":"IPY_MODEL_bf2a63976bd6400ab3e541ed2a9d5802"}},"bc496fbacdef434db21c7a614728fd59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4e5d5c6d70044629be88faea927fde7","placeholder":"​","style":"IPY_MODEL_ca1c1009f026404391b0a62a6acba693","value":"100%"}},"445ecf6ce5b244038938398d2f1cacdd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cfec95523a042f7bc5536c0ea2109df","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7efed9a03d2b4ab286b8c92948f2f77b","value":63}},"c854a3b7152f4a43b146fd80904cb9f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03afd4d04a324576b65a62a6a2f956d4","placeholder":"​","style":"IPY_MODEL_90c406ec888947c3af6d75c26a6322c6","value":" 63/63 [00:47&lt;00:00,  1.49it/s]"}},"bf2a63976bd6400ab3e541ed2a9d5802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4e5d5c6d70044629be88faea927fde7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca1c1009f026404391b0a62a6acba693":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cfec95523a042f7bc5536c0ea2109df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7efed9a03d2b4ab286b8c92948f2f77b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03afd4d04a324576b65a62a6a2f956d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90c406ec888947c3af6d75c26a6322c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e111b9924f94ebb8f58567f49f46384":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_519340a6577a4edfa1b2ddfb2c1ede43","IPY_MODEL_dea0a7b1db514d328b96cd2a09baa4d9","IPY_MODEL_e8310516f531423b825b72540d7571fa"],"layout":"IPY_MODEL_aace4271a55944b4a15fe97dc55c3895"}},"519340a6577a4edfa1b2ddfb2c1ede43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83509ffa228743bc9826878186585558","placeholder":"​","style":"IPY_MODEL_d373fded8b8c41f19675ce1b1c5e1c99","value":"100%"}},"dea0a7b1db514d328b96cd2a09baa4d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bd42fb6928e40c1a63b3633591fa41c","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1db1fd04e720478e9415b228288aabc4","value":63}},"e8310516f531423b825b72540d7571fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb06702a23e940c684568ca56ec59223","placeholder":"​","style":"IPY_MODEL_9987486369064fb2b1259ec39956c9fb","value":" 63/63 [01:19&lt;00:00,  1.08s/it]"}},"aace4271a55944b4a15fe97dc55c3895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83509ffa228743bc9826878186585558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d373fded8b8c41f19675ce1b1c5e1c99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bd42fb6928e40c1a63b3633591fa41c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db1fd04e720478e9415b228288aabc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb06702a23e940c684568ca56ec59223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9987486369064fb2b1259ec39956c9fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdec9f2f21db475ea081cf0e7a997728":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f06ecbe32a054ea292bbf2b6ebffed76","IPY_MODEL_cf656c7639104dddaa2b6f5069bf9c7a","IPY_MODEL_f6a5c9bfbd7e4ef9b791e534842fdb5f"],"layout":"IPY_MODEL_33876344f46f43459ad7df5b86371ff3"}},"f06ecbe32a054ea292bbf2b6ebffed76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d0e162e51ae4d8795b2c0c071bc6306","placeholder":"​","style":"IPY_MODEL_1cd27fceeabd4524b3030890d3f13529","value":"Map (num_proc=4): 100%"}},"cf656c7639104dddaa2b6f5069bf9c7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eb047db199449cbb749fc7fccf9b4ad","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62cbb07ff7244c82b4ba3782f93a8274","value":10000}},"f6a5c9bfbd7e4ef9b791e534842fdb5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c396d4c975b4755b962ec31cdb68594","placeholder":"​","style":"IPY_MODEL_22ae5f8af4304f4c9f7cecd127a421a1","value":" 10000/10000 [00:06&lt;00:00, 1787.87 examples/s]"}},"33876344f46f43459ad7df5b86371ff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d0e162e51ae4d8795b2c0c071bc6306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cd27fceeabd4524b3030890d3f13529":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2eb047db199449cbb749fc7fccf9b4ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62cbb07ff7244c82b4ba3782f93a8274":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c396d4c975b4755b962ec31cdb68594":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22ae5f8af4304f4c9f7cecd127a421a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05e5b5db2043429e924e89855d0f0a84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df2a1a64dfa84edbb89adc00b77a135d","IPY_MODEL_4bd4cb0467884fad8ea412d126948e7c","IPY_MODEL_2bd39897984e4821bf39ed38b00c5443"],"layout":"IPY_MODEL_298bb1e5e1064935867c0ae1a3ea064d"}},"df2a1a64dfa84edbb89adc00b77a135d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b20dcf718f43e69f764ea1c6a3200d","placeholder":"​","style":"IPY_MODEL_a234735800eb43349e3de43e8dd85f95","value":"Map (num_proc=4): 100%"}},"4bd4cb0467884fad8ea412d126948e7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7552e6e77dcf47e5b2809f9c8c8e6942","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14d05dbaab7f4052a153b5e679528735","value":500}},"2bd39897984e4821bf39ed38b00c5443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a4995c046784061a30635df41bb7b95","placeholder":"​","style":"IPY_MODEL_56914d68eff7401cb65fe43a7293ecf0","value":" 500/500 [00:00&lt;00:00, 1173.26 examples/s]"}},"298bb1e5e1064935867c0ae1a3ea064d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b20dcf718f43e69f764ea1c6a3200d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a234735800eb43349e3de43e8dd85f95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7552e6e77dcf47e5b2809f9c8c8e6942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14d05dbaab7f4052a153b5e679528735":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a4995c046784061a30635df41bb7b95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56914d68eff7401cb65fe43a7293ecf0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f98d4c9a0674a258161c7537a42fb13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_312f1477445d4b6d862bf6875ad08651","IPY_MODEL_f5b9506ab6104e09a6994a4c8efe4846","IPY_MODEL_5478a67c333d4de89b522ce611d0e57a"],"layout":"IPY_MODEL_aea9d7797d954a3586c480ae57f76596"}},"312f1477445d4b6d862bf6875ad08651":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59a52aa9700a41629b157fd49bd03ef7","placeholder":"​","style":"IPY_MODEL_2635c9860e8c44dfa775331db4a725b7","value":"Map (num_proc=4): 100%"}},"f5b9506ab6104e09a6994a4c8efe4846":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6666ec9bc9a544448004a47d714d2588","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48e906f4b4af447ca55e45844ecf75c6","value":1000}},"5478a67c333d4de89b522ce611d0e57a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75e8b6e205934aeaa24d6b9a88f4ffa3","placeholder":"​","style":"IPY_MODEL_032a6aa2299342e9965da2369bbbf3be","value":" 1000/1000 [00:00&lt;00:00, 522.61 examples/s]"}},"aea9d7797d954a3586c480ae57f76596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59a52aa9700a41629b157fd49bd03ef7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2635c9860e8c44dfa775331db4a725b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6666ec9bc9a544448004a47d714d2588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48e906f4b4af447ca55e45844ecf75c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75e8b6e205934aeaa24d6b9a88f4ffa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"032a6aa2299342e9965da2369bbbf3be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55d343f194ea433b902a36f47666a297":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4acc352f117840aa8dba9ddd4ccfc81f","IPY_MODEL_eb0fd203f99b47398186f76d6528500a","IPY_MODEL_c4f3f7290fac4d1f99b6fc2b24c2d5db"],"layout":"IPY_MODEL_3f9320ec66b74fc1aa6bf704e05ab64c"}},"4acc352f117840aa8dba9ddd4ccfc81f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_253f0fee802e4e6aa0b4b06565728a36","placeholder":"​","style":"IPY_MODEL_ca08f1fb447e4e6884374fc41cc8d2a9","value":"Map (num_proc=4): 100%"}},"eb0fd203f99b47398186f76d6528500a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_29ecbda152294fabb2a476e1be34da95","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f7787d08a1442338713956aabbbff3a","value":10000}},"c4f3f7290fac4d1f99b6fc2b24c2d5db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e80a4b8f0fe543a8a01b152c3741939f","placeholder":"​","style":"IPY_MODEL_e4aefa0a70314b799035fa2a2a0429fc","value":" 10000/10000 [00:06&lt;00:00, 1913.25 examples/s]"}},"3f9320ec66b74fc1aa6bf704e05ab64c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253f0fee802e4e6aa0b4b06565728a36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca08f1fb447e4e6884374fc41cc8d2a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29ecbda152294fabb2a476e1be34da95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f7787d08a1442338713956aabbbff3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e80a4b8f0fe543a8a01b152c3741939f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4aefa0a70314b799035fa2a2a0429fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e4d66441f0e4eb8848081bf2d12d468":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16717142a2b94afdb9abd8902507fd82","IPY_MODEL_e51707e7caa74784a39443cd04a8dffd","IPY_MODEL_9ae46aefada348e0b6c8c3b6dc531e34"],"layout":"IPY_MODEL_fa60d77d4a954bf687a54ae27ba80291"}},"16717142a2b94afdb9abd8902507fd82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91f085759fe54812a24d0da509b19f41","placeholder":"​","style":"IPY_MODEL_68d94b51b75648458abd53a677675d3c","value":"Map (num_proc=4): 100%"}},"e51707e7caa74784a39443cd04a8dffd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18d5f9e8627844df8d6f8695fcb9d8c2","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc68a8dc5d5846a184940c3d07d93073","value":500}},"9ae46aefada348e0b6c8c3b6dc531e34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c91d1855d2844cc180ea4e0d0630dea6","placeholder":"​","style":"IPY_MODEL_cd45a209d42e442c82473658434d835a","value":" 500/500 [00:00&lt;00:00, 206.89 examples/s]"}},"fa60d77d4a954bf687a54ae27ba80291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91f085759fe54812a24d0da509b19f41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68d94b51b75648458abd53a677675d3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18d5f9e8627844df8d6f8695fcb9d8c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc68a8dc5d5846a184940c3d07d93073":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c91d1855d2844cc180ea4e0d0630dea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd45a209d42e442c82473658434d835a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4d11ea6f4a34f9ca469d4e0ca60a210":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98dfd0b91af643b48f4a898d029a5147","IPY_MODEL_328bec1a47824ec0b8b7e52f2ac873e5","IPY_MODEL_e7de55aa81284c1994064b7e4390133f"],"layout":"IPY_MODEL_f409deee3ecb4d9f96d1bdf1f17374e2"}},"98dfd0b91af643b48f4a898d029a5147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_705a6c6cf2c34b18b8420df13de184ad","placeholder":"​","style":"IPY_MODEL_533639e655ba4af5babc4649b6066394","value":"Map (num_proc=4): 100%"}},"328bec1a47824ec0b8b7e52f2ac873e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e1bfa217964f23a4ee14c17b271d6a","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2ae50c832a74456acc48749788db2dc","value":1000}},"e7de55aa81284c1994064b7e4390133f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61acfd39c31d4724a064faf094a2f1d8","placeholder":"​","style":"IPY_MODEL_0079eb5dadbf4751a41b6b795e4041d9","value":" 1000/1000 [00:00&lt;00:00, 880.78 examples/s]"}},"f409deee3ecb4d9f96d1bdf1f17374e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"705a6c6cf2c34b18b8420df13de184ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"533639e655ba4af5babc4649b6066394":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0e1bfa217964f23a4ee14c17b271d6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2ae50c832a74456acc48749788db2dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61acfd39c31d4724a064faf094a2f1d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0079eb5dadbf4751a41b6b795e4041d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"492ce003eef64ac3ac80dac612549801":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ed14c9a805f4e7eb8246d2ea88306a0","IPY_MODEL_14e490887b374de785ff3f197178d114","IPY_MODEL_e199e46a27b746a19720c30a2b10b7cf"],"layout":"IPY_MODEL_92333320977c4be2881265fa03b543bc"}},"4ed14c9a805f4e7eb8246d2ea88306a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5dc741290714eae8bc275fad3f010af","placeholder":"​","style":"IPY_MODEL_af593615025f4ab8a78e0f10001a4738","value":"100%"}},"14e490887b374de785ff3f197178d114":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9ee611c928f450db9430eecc0e494aa","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35bb0c021a4f4a5ca347b5f3b844a079","value":63}},"e199e46a27b746a19720c30a2b10b7cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d78c6d0322804c18bb09fb426d63b0ab","placeholder":"​","style":"IPY_MODEL_7ff172e076b740b6b8ed2ac118f8d6a2","value":" 63/63 [01:20&lt;00:00,  1.09s/it]"}},"92333320977c4be2881265fa03b543bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5dc741290714eae8bc275fad3f010af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af593615025f4ab8a78e0f10001a4738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9ee611c928f450db9430eecc0e494aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35bb0c021a4f4a5ca347b5f3b844a079":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d78c6d0322804c18bb09fb426d63b0ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ff172e076b740b6b8ed2ac118f8d6a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"708b7aa0216c4551b27de8db36029795":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_081295bb7a024a67848013e7138a6286","IPY_MODEL_2684491d194e496eb756dafb2b01a362","IPY_MODEL_b497260337d04c9e90f1860edc7bf8d5"],"layout":"IPY_MODEL_dcba8d04bd5e4278b73bbbb6c63163c1"}},"081295bb7a024a67848013e7138a6286":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d667cdf69bbd4c18be8903c4b6aa5165","placeholder":"​","style":"IPY_MODEL_5652c3d566684fff922dc81133cc35bd","value":"100%"}},"2684491d194e496eb756dafb2b01a362":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fb3b0b8b4b14e7caa8e56504a71c733","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8399c5c0f2a6407a9d5267e36eba17d3","value":1}},"b497260337d04c9e90f1860edc7bf8d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84be9d87031c459e9faadf228c995b4a","placeholder":"​","style":"IPY_MODEL_3477d5f5f70f4aed85cdda7b2e851687","value":" 1/1 [00:00&lt;00:00, 23.89it/s]"}},"dcba8d04bd5e4278b73bbbb6c63163c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d667cdf69bbd4c18be8903c4b6aa5165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5652c3d566684fff922dc81133cc35bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fb3b0b8b4b14e7caa8e56504a71c733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8399c5c0f2a6407a9d5267e36eba17d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84be9d87031c459e9faadf228c995b4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3477d5f5f70f4aed85cdda7b2e851687":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b48e11584414824a2312e3d8b1dce3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00f0ad8bd05f4cc9a66d553ac8b98c18","IPY_MODEL_77be0c835aaf487a89e9dc931e5cc1f4","IPY_MODEL_175317bd9d3c4129a05415c1232cac4a"],"layout":"IPY_MODEL_6d0144141c8e413da53755ada397a41f"}},"00f0ad8bd05f4cc9a66d553ac8b98c18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db1e9b62c7de45949f6cc5a0a3b334c1","placeholder":"​","style":"IPY_MODEL_f31f487031a64cfe942b8f2ff689f094","value":"100%"}},"77be0c835aaf487a89e9dc931e5cc1f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb6052391854359ae6a1c3f91f86066","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3af00981fb514fa6a32771c5f1d4dc4f","value":1}},"175317bd9d3c4129a05415c1232cac4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58f54cffb3f9474888bc4989a6ef00fe","placeholder":"​","style":"IPY_MODEL_b2db71f3802a4b7898a68f2f58c22b03","value":" 1/1 [00:00&lt;00:00, 16.63it/s]"}},"6d0144141c8e413da53755ada397a41f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db1e9b62c7de45949f6cc5a0a3b334c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f31f487031a64cfe942b8f2ff689f094":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb6052391854359ae6a1c3f91f86066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3af00981fb514fa6a32771c5f1d4dc4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58f54cffb3f9474888bc4989a6ef00fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2db71f3802a4b7898a68f2f58c22b03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e212c8931aa49c8a80d78de82d398a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4019aff066bb491eb1d9f1f96c3da498","IPY_MODEL_3f7776a8bee74cc0bd7dfa19945e00c8","IPY_MODEL_373b0a56e2dc41ba959060161e281476"],"layout":"IPY_MODEL_3d82c2047ec0476cbdd54cb2350530d5"}},"4019aff066bb491eb1d9f1f96c3da498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a031e38448d4820ab4e2625b3362a03","placeholder":"​","style":"IPY_MODEL_18881968593e41b0a9ee34949828293a","value":"100%"}},"3f7776a8bee74cc0bd7dfa19945e00c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fda8b88ea6974851a9fae71085ff4220","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c58c25ccb346411986473c6bec04673f","value":63}},"373b0a56e2dc41ba959060161e281476":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17c2134c57704a8b8ec19abc9d94ed9d","placeholder":"​","style":"IPY_MODEL_d008032ba5374f8699251233fbf43826","value":" 63/63 [01:20&lt;00:00,  1.09s/it]"}},"3d82c2047ec0476cbdd54cb2350530d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a031e38448d4820ab4e2625b3362a03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18881968593e41b0a9ee34949828293a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fda8b88ea6974851a9fae71085ff4220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c58c25ccb346411986473c6bec04673f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17c2134c57704a8b8ec19abc9d94ed9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d008032ba5374f8699251233fbf43826":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9415167,"sourceType":"datasetVersion","datasetId":5717891},{"sourceId":9418581,"sourceType":"datasetVersion","datasetId":5720386}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment 1: Build a Toy Llama-2 Language Model\n\n> CISC7021 Applied Natural Language Processing (2024/2025)\n\nIn this assignment, we will prepare a toy language model that employs the **Llama-2** architecture and evaluate the perplexity of the data set.\n\nWe will learn how to perform continual pre-training of a base language model using the PyTorch and Hugging Face libraries. Detailed instructions for building this language model can be found in the attached notebook file.\n\nAcknowledgement: The base model checkpoint is converted from [llama2.c](https://github.com/karpathy/llama2.c) project. The data instances were sampled from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.\n\n---\n\n🚨 Please note that running this on CPU may be slow. If running on Google Colab or Kaggle, you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n\n---\n\nWe start by doing a `pip install` of all required libraries.\n- 🤗 `transformers`, `datasets`, `accelerate` are Huggingface libraries.\n- By default, Colab has `transformers`, `pytorch` libraries installed. If you are using a local machine, please install them via `pip` or `conda`.","metadata":{"id":"qycf3ikvkUqP"}},{"cell_type":"code","source":"#!pip install torch torchvision torchaudio\n#!pip install transformers","metadata":{"id":"UOhVvTEaa_b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets accelerate -q","metadata":{"id":"mNC4vO-JkUqQ","executionInfo":{"status":"ok","timestamp":1726504849030,"user_tz":-480,"elapsed":11122,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8f2ad61-b770-4886-870c-0dd0c7e4f98f","execution":{"iopub.status.busy":"2024-09-16T19:48:49.992584Z","iopub.execute_input":"2024-09-16T19:48:49.992923Z","iopub.status.idle":"2024-09-16T19:49:24.755334Z","shell.execute_reply.started":"2024-09-16T19:48:49.992891Z","shell.execute_reply":"2024-09-16T19:49:24.754260Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### (Optional) Uploading the model/data to Google Colab or Kaggle.\n\nPlease upload your dataset and model to computational platforms if you are using Colab or Kaggle environments.\n\nFor Colab users, you can mount your Google Drive files by running the following code snippet:","metadata":{"id":"2WzqztoXkUqQ"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"kRSpL18W_Zfa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726514702196,"user_tz":-480,"elapsed":22017,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"5bdfd93f-a25c-4e7a-f79c-f4740de4ae7e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"code","source":"!pip install pyarrow==15.0.2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kzneLKEWR44","executionInfo":{"status":"ok","timestamp":1726504833915,"user_tz":-480,"elapsed":8608,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"66ccdea2-8803-40ea-c8a4-826b81912d82","execution":{"iopub.status.busy":"2024-09-16T19:49:24.757240Z","iopub.execute_input":"2024-09-16T19:49:24.757619Z","iopub.status.idle":"2024-09-16T19:50:55.534297Z","shell.execute_reply.started":"2024-09-16T19:49:24.757586Z","shell.execute_reply":"2024-09-16T19:50:55.533122Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc2bbf7dd50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyarrow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc2bbf7ded0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/pyarrow/\u001b[0m\u001b[33m\n\u001b[0mCollecting pyarrow==15.0.2\n  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<2,>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow==15.0.2) (1.26.4)\nDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 16.1.0\n    Uninstalling pyarrow-16.1.0:\n      Successfully uninstalled pyarrow-16.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.2 requires cubinlinker, which is not installed.\ncudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.4 which is incompatible.\ncudf 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ncudf 24.8.2 requires pyarrow<16.2.0a0,>=16.1.0, but you have pyarrow 15.0.2 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.2 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-15.0.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Necessary Packages, Environment Setups","metadata":{"id":"dqvZRLZYkUqR"}},{"cell_type":"code","source":"import torch\nimport transformers\n\nfrom typing import List, Optional, Tuple, Union\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\nfrom transformers import Trainer, TrainingArguments\nfrom itertools import chain\nfrom datasets import load_dataset\n\nfrom tqdm.notebook import tqdm\nfrom torch.nn import CrossEntropyLoss","metadata":{"id":"Sa1iUH1ykUqR","executionInfo":{"status":"ok","timestamp":1726509405834,"user_tz":-480,"elapsed":24528,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T09:11:58.312033Z","iopub.execute_input":"2024-09-17T09:11:58.312687Z","iopub.status.idle":"2024-09-17T09:12:17.935577Z","shell.execute_reply.started":"2024-09-17T09:11:58.312653Z","shell.execute_reply":"2024-09-17T09:12:17.934799Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Please set the correct file path based on your environment.\n\n- If you are using Colab, the path may be: `/content/drive/MyDrive/xxxxxx`\n- If you are using Kaggle, the path may be: `/kaggle/input/xxxxxx`","metadata":{"id":"TBzFKWGZkUqR"}},{"cell_type":"code","source":"# Please set the correct file path based on your environment.\nTRAIN_FILE = '/kaggle/input/assignment1/assignment1/data/zh_train.jsonl'\nVALIDATION_FILE = '/kaggle/input/assignment1/assignment1/data/zh_dev.jsonl'\nTEST_FILE = '/kaggle/input/assignment1/assignment1/data/zh_test.jsonl'\nEN_TEST_FILE = '/kaggle/input/assignment1/assignment1/data/en_test.jsonl'\nMODEL_FOLDER = \"/kaggle/input/assignment1/assignment1/llama-42m\"","metadata":{"id":"mYj0DvSgGmWq","executionInfo":{"status":"ok","timestamp":1726509409710,"user_tz":-480,"elapsed":606,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T09:12:25.842286Z","iopub.execute_input":"2024-09-17T09:12:25.843000Z","iopub.status.idle":"2024-09-17T09:12:25.847805Z","shell.execute_reply.started":"2024-09-17T09:12:25.842959Z","shell.execute_reply":"2024-09-17T09:12:25.846775Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Load the model checkpoint into either a GPU or CPU (training will be slow on CPU, but decoding will be fair).","metadata":{"id":"ElstHAMjkUqS"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device type: {device}\")\n\nmodel_path = MODEL_FOLDER\n# Load model from local files\nmodel = LlamaForCausalLM.from_pretrained(model_path).to(device)\n# Load tokenizer from local files\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZF-0tQYDjvPl","outputId":"aa82df6b-6d34-4113-fd17-b8806fae1081","executionInfo":{"status":"ok","timestamp":1726514238859,"user_tz":-480,"elapsed":467,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T09:12:36.077384Z","iopub.execute_input":"2024-09-17T09:12:36.078131Z","iopub.status.idle":"2024-09-17T09:12:38.841773Z","shell.execute_reply.started":"2024-09-17T09:12:36.078066Z","shell.execute_reply":"2024-09-17T09:12:38.840966Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see from the statistics, this model is much smaller than Llama-2 but shares the same decoder-only architecture.\n\n\n😄 **You do not need to check complex details!** We just present the architecture and number of parameters here.","metadata":{"id":"0g81Fac6kUqS"}},{"cell_type":"code","source":"total_para = sum(v.numel() for k, v in model.state_dict().items() if k != 'model.embed_tokens.weight') / 1e6\nprint(model)\nprint(f\"#Parameters: {total_para:.2f}M\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dqxUbbA2u2uc","outputId":"9222be9a-a09a-4287-9410-630616baea89","executionInfo":{"status":"ok","timestamp":1726514242668,"user_tz":-480,"elapsed":410,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T09:12:41.471875Z","iopub.execute_input":"2024-09-17T09:12:41.472260Z","iopub.status.idle":"2024-09-17T09:12:41.480506Z","shell.execute_reply.started":"2024-09-17T09:12:41.472222Z","shell.execute_reply":"2024-09-17T09:12:41.479631Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 512)\n    (layers): ModuleList(\n      (0-7): 8 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((512,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n)\n#Parameters: 41.69M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 1: Decoding","metadata":{"id":"Z6LPYv5KkUqS"}},{"cell_type":"markdown","source":"\nIf you are familar with the usage of `model.generate()` function in transformer library, please feel free to jump to [Task 1 Playground](#scrollTo=Task_1_Playground).\n\n\n#### 💡Tutorials: model.generate() function.\n---\nMinimal example:\n\n```python\nprompt = \"Once upon a time, \" # Input, prefix of generation\n```\n\n**Step 1**: Encode raw text using tokenizer model.\n```python\ntokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n```\n\n**Step 2**: Set decoding hyper-parameters. Get the model output.\n```python\noutput_ids = model.generate(tokenized_input, do_sample=True, max_new_tokens=300, temperature=0.6)\n```\nImportant parameters:\n- `max_new_tokens`: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n- `temperature`: The value of temperature used to modulate the next token probabilities. Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n- `do_sample`: `do_sample=False` is using greedy decoing strategy. To enable greedy decoding, we also need to set other sampling parameters `top_p`, `temperature` as `None`.\n- [If you are interested in other decoding algorithms, please refer to this link for setting parameters.](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationConfig)\n\n**Step 3**: Convert model outputs into raw text.\n```python\noutput_text = tokenizer.decode(output_ids[0])\n```\nor (when input instances >=1)\n```python\noutput_text = tokenizer.batch_decode(output_ids)\n```\nImportant parameters:\n- Setting `skip_special_tokens=True` will prevent special tokens, such as `<s>`, from appearing in the results..\n\n---\n","metadata":{"id":"PxuzZuOrR4mE"}},{"cell_type":"markdown","source":"To understand the outputs of each step, let us do a simple generation task step by step! (Note: the base model is only able to produce fluent story text).","metadata":{"id":"tkvkY3LPkUqT"}},{"cell_type":"code","source":"prompt = \"Once upon a time, Stella Lou had a dream.\" # Feel free to use other generation prefix","metadata":{"id":"Gf6Q9qcgkUqT","execution":{"iopub.status.busy":"2024-09-16T20:02:45.308552Z","iopub.execute_input":"2024-09-16T20:02:45.308980Z","iopub.status.idle":"2024-09-16T20:02:45.313693Z","shell.execute_reply.started":"2024-09-16T20:02:45.308941Z","shell.execute_reply":"2024-09-16T20:02:45.312735Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Step 1: Encode raw text using tokenizer model. Run tokenization and covert strings into token ids in vocabulary.\ntokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n# See the tokenized results.\nprint(tokenized_input)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEKNYuWiJKIB","outputId":"beb5c3f7-660f-4094-df62-88c6ae9eb0df","executionInfo":{"status":"ok","timestamp":1726499063603,"user_tz":-480,"elapsed":726,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:02:47.175099Z","iopub.execute_input":"2024-09-16T20:02:47.175460Z","iopub.status.idle":"2024-09-16T20:02:47.184411Z","shell.execute_reply.started":"2024-09-16T20:02:47.175429Z","shell.execute_reply":"2024-09-16T20:02:47.183565Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n           263, 12561, 29889]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 2: Set decoding hyperparameters.\n\n# For greedy decoding\nmax_new_tokens = 300\ndo_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\ntemperature = None\n\n# call generation function model.generate()\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=None,\n)\n\n# The decoded results are token ids.\nprint(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\nprint(output_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlmCc71dElxz","outputId":"850b6a5c-6f67-43ee-b765-4a106902dea3","executionInfo":{"status":"ok","timestamp":1726499078551,"user_tz":-480,"elapsed":4370,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:02:49.367977Z","iopub.execute_input":"2024-09-16T20:02:49.368988Z","iopub.status.idle":"2024-09-16T20:02:51.440210Z","shell.execute_reply.started":"2024-09-16T20:02:49.368943Z","shell.execute_reply":"2024-09-16T20:02:51.439227Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"====================Token IDs====================\ntensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n           263, 12561, 29889,  2296,  5131,   304,   367,   263, 12456,   985,\n         29889,  2296,  5131,   304, 19531,   263,  9560, 10714,   322,   263,\n           528,  4901, 20844, 29889,  1205,  1183,   471,  2086,  2319,   322,\n           278, 10714,   471,  2086,  4802, 29889,    13,  6716,  2462, 29892,\n           624,  3547,  4446,   263,  4802, 29892,   528,  4901, 10714,   297,\n           263,  3787, 29889,  2296,  4433,   902, 16823,   565,  1183,  1033,\n           505,   372, 29889,  2439, 16823,  1497,  4874,   322, 18093,   372,\n           363,   902, 29889,    13,   855,  3547,   471,   577,  9796, 29889,\n          2296,  1925,   373,   278, 10714,   322,  3252,   381,   839,  2820,\n         29889,  2296,  7091,   763,   263,  1855, 12456,   985, 29889,    13,\n          6246,   769, 29892,  1554,  8515,  9559, 29889,   624,  3547,  4687,\n           304,  4459,   270,   466,  1537, 29889,  2296,  8496, 29915, 29873,\n          2317,   701,  7812, 29889,  2296,  7091,   763,  1183,   471, 10917,\n          1076,  2820,   322,  2820, 29889,    13,   855,  3547, 29915, 29879,\n         16823,  4446,   902,   322,  1497, 29892,   376,   855,  3547, 29892,\n           366,   817,   304,  2125,   263,  2867, 29889,   887,  1106,   270,\n           466,  1537,  1213,    13,   855,  3547,  3614,  1283,   278, 10714,\n           322,  6568,  1623,   373,   278, 11904, 29889,  2296,  5764,   902,\n          5076,   322,  3614,   263,  6483, 16172, 29889,  2860,   263,  2846,\n          6233, 29892,  1183,  7091,  2253, 29889,    13,   855,  3547, 25156,\n           322,  1497, 29892,   376, 29924,   290, 29892,   306, 29915, 29885,\n          7960,   304,   367,   263, 12456,   985,  1449,  3850,     1]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 3: Convert model outputs into raw text.\n# decode token ids into tokens\nprint(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\n# We only have one input instance. So we directly decode the first item of model output, i.e., `output_ids[0]`.\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQqRmKrXIYM1","outputId":"12e3ae5a-6e42-4d7d-ef5a-87e891f39de0","executionInfo":{"status":"ok","timestamp":1726499086862,"user_tz":-480,"elapsed":644,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:02:54.243849Z","iopub.execute_input":"2024-09-16T20:02:54.244571Z","iopub.status.idle":"2024-09-16T20:02:54.250760Z","shell.execute_reply.started":"2024-09-16T20:02:54.244531Z","shell.execute_reply":"2024-09-16T20:02:54.249768Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"====================Decoded Results====================\nOnce upon a time, Stella Lou had a dream. She wanted to be a princess. She wanted to wear a beautiful dress and a shiny crown. But she was too small and the dress was too big.\nOne day, Stella saw a big, shiny dress in a store. She asked her mom if she could have it. Her mom said yes and bought it for her.\nStella was so happy. She put on the dress and twirled around. She felt like a real princess.\nBut then, something strange happened. Stella started to feel dizzy. She couldn't stand up straight. She felt like she was spinning around and around.\nStella's mom saw her and said, \"Stella, you need to take a break. You look dizzy.\"\nStella took off the dress and lay down on the floor. She closed her eyes and took a deep breath. After a few minutes, she felt better.\nStella smiled and said, \"Mom, I'm ready to be a princess again!\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Another pipeline example: Sampling decoding with temperature.","metadata":{"id":"pRNuqdJkRvxw"}},{"cell_type":"code","source":"prompt = \"Once upon a time, Stella Lou had a dream.\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\n# The value of temperature used to modulate the next token probabilities.\n# Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCqO5bkl341O","outputId":"bad5340c-a6e7-4ce0-d4eb-389f0ea80c29","execution":{"iopub.status.busy":"2024-09-16T20:02:56.509387Z","iopub.execute_input":"2024-09-16T20:02:56.509765Z","iopub.status.idle":"2024-09-16T20:02:59.138457Z","shell.execute_reply.started":"2024-09-16T20:02:56.509729Z","shell.execute_reply":"2024-09-16T20:02:59.137497Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<s> Once upon a time, Stella Lou had a dream. She wanted to be a princess and live in a castle. She was so excited to see what the world had to offer.\nOne day, Stella went to the park and saw a beautiful princess. She was so happy and ran up to her. The princess said, \"Hello Stella, I am the princess of this land. Would you like to be my friend?\"\nStella was so excited and said, \"Yes, I would love to be your friend!\"\nThe princess smiled and said, \"Let's go to my castle and have a tea party. We can have a tea party and eat yummy treats.\"\nStella was so happy and said, \"Yes, let's do that!\"\nSo the princess and Stella went to the castle and had a wonderful tea party. They laughed and talked and had a wonderful time. Stella was so happy to have a new friend.\nThe princess said, \"You are so special Stella Lou. I am so glad you are my friend.\"\nStella Lou smiled and said, \"I am so glad too. I am so happy to have you as my friend.\"\nThe princess smiled and said, \"I am so happy to have you as my friend too.\"\nAnd they lived happily ever after.<s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Task 1 Playground\n\n---\n\n📚 Task 1: Please generate English stories using various prompts and decoding settings. Please feel free to explore any interesting phenomena, such as the impact of different prompts and the effects of various decoding algorithms and parameters. For example, quantify the text properties using linguistic-driven metrics like story length and Type-Token Ratio (TTR). In addition to objective metrics, you are encouraged to discuss your findings based on subjective case studies.\n\nWe provide two types of skeleton code: one that takes a single prompt as input and another that can process batched inputs and decoding. Please use the version that best fits your preferences and data types.\n\n---","metadata":{"id":"LT7or09aSBhp"}},{"cell_type":"code","source":"prompt = \"Blair is a cute little girl lived in a happy family.\"","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:02:39.279905Z","iopub.execute_input":"2024-09-17T06:02:39.280637Z","iopub.status.idle":"2024-09-17T06:02:39.285436Z","shell.execute_reply.started":"2024-09-17T06:02:39.280596Z","shell.execute_reply":"2024-09-17T06:02:39.283828Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\nprint(tokenized_input)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:02:40.411179Z","iopub.execute_input":"2024-09-17T06:02:40.411558Z","iopub.status.idle":"2024-09-17T06:02:40.428987Z","shell.execute_reply.started":"2024-09-17T06:02:40.411522Z","shell.execute_reply":"2024-09-17T06:02:40.428086Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[    1, 10465,   381,   338,   263,   274,  1082,  2217,  7826, 10600,\n           297,   263,  9796,  3942, 29889]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# For greedy decoding\nmax_new_tokens = 300\ndo_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\ntemperature = None\n\n# call generation function model.generate()\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=None,\n)\n\n# The decoded results are token ids.\nprint(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\nprint(output_ids)\n\nprint(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:04:22.825709Z","iopub.execute_input":"2024-09-17T06:04:22.826569Z","iopub.status.idle":"2024-09-17T06:04:24.417826Z","shell.execute_reply.started":"2024-09-17T06:04:22.826524Z","shell.execute_reply":"2024-09-17T06:04:24.416829Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"====================Token IDs====================\ntensor([[    1, 10465,   381,   338,   263,   274,  1082,  2217,  7826, 10600,\n           297,   263,  9796,  3942, 29889,  7569,  2462, 29892,  1183,   723,\n           748,  5377,   322,  1708,   297,   278, 16423, 29889,  3118,  2462,\n         29892,  1183,  4446,   263,  4802, 29892, 13328,   541,   357, 17652,\n         29889,  2296,  5131,   304,  4380,   372, 29892,   577,  1183,  6350,\n          1156,   372, 29889,    13,  1576,   541,   357, 17652,  9115, 29893,\n          3448,   322,  5331,   902,   304,   263,  4802,  5447, 29889,   739,\n           471,   577,  4802, 29892,   372,  6140,   304,   367, 25508,  1554,\n         29889,  2296,  5148,  2820,   322,  4446,   263,  4802, 29892, 13328,\n         28149, 29889,  2296,   471,   577, 24173,   322,  6350,   304,  5839,\n           372, 29889,    13,  6246,   746,  1183, 23051,   372, 29892,   278,\n         28149,  4687,   304,  4337, 29991,   739,   471,   263,   274,  1008,\n         29886,   453,   279, 29991,   739,   471,   577,   274,  1082, 29892,\n           322,   372,   471,   577,  9796,   304,  1074,   902, 29889,    13,\n         29933,   433,   381,   471,   577, 18014, 29892,  1183,  8496, 29915,\n         29873,  4658,   372, 29889,  2296,   750,  2360,  3595,   263,   274,\n          1008, 29886,   453,   279,  1434, 29889,  2296, 25156,   322,  1497,\n         22172, 29889,   450,   274,  1008, 29886,   453,   279,  1497, 22172,\n          1250,   322,   896,  3897,  7875, 29889,    13, 29933,   433,   381,\n           322,   278,   274,  1008, 29886,   453,   279,  5318,  4208,  1432,\n          2462, 29889,  2688,   750,   577,  1568,  2090, 29991,     1]],\n       device='cuda:0')\n====================Decoded Results====================\nBlair is a cute little girl lived in a happy family. Every day, she would go outside and play in the garden. One day, she saw a big, yellow butterfly. She wanted to catch it, so she ran after it.\nThe butterfly flew away and led her to a big tree. It was so big, it seemed to be hiding something. She looked around and saw a big, yellow flower. She was so excited and ran to pick it.\nBut when she touched it, the flower started to move! It was a caterpillar! It was so cute, and it was so happy to see her.\nBlair was so surprised, she couldn't believe it. She had never seen a caterpillar before. She smiled and said hello. The caterpillar said hello back and they became friends.\nBlair and the caterpillar played together every day. They had so much fun!\n","output_type":"stream"}]},{"cell_type":"code","source":"max_new_tokens = 300\ndo_sample = True\ntemperature = 0.5\ntop_p = 0.95\n\n# call generation function model.generate()\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=None,\n)\n\n# The decoded results are token ids.\nprint(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\nprint(output_ids)\n\nprint(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:04:42.934051Z","iopub.execute_input":"2024-09-17T06:04:42.934746Z","iopub.status.idle":"2024-09-17T06:04:44.988643Z","shell.execute_reply.started":"2024-09-17T06:04:42.934704Z","shell.execute_reply":"2024-09-17T06:04:44.987446Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"====================Token IDs====================\ntensor([[    1, 10465,   381,   338,   263,   274,  1082,  2217,  7826, 10600,\n           297,   263,  9796,  3942, 29889,  2296,   471,  2337,  8743,   411,\n           902,   304,   952,   322,  2734,  2820,   278,  3699, 29889,  3118,\n          2462, 29892,  1183,   471,  8743,   411,   902,   304, 29891,  1559,\n           746,  1183,  6091,   263, 22526, 11462, 29889,  2296,  5148,   701,\n           322,  4446,   263,  4802, 29892,  4628,  9570,   297,   278, 14744,\n         29889,    13, 29924,   290,  1357,  1497, 29892,   376,  4806,   817,\n           304,   748,   304,   278, 11619, 29892, 22827,  3850,    13, 29933,\n           433,   381,   471,   885,  1965, 29892,   541,  1183,  6363,   393,\n           341,   290,  1357,   471,  1492, 29889,  2688,  3512,   304,   278,\n         11619,   322,   540,  1497,   393,   278,  9570,   471,   263,  4319,\n         14280, 29889,   940,  1497,   896,  4312,   304,  7952,  2768,  2745,\n           278, 14280,   471,   975, 29889,    13, 29933,   433,   381,   471,\n         14610, 29892,   541,   341,   290,  1357,  1497, 29892,   376, 10310,\n         29915, 29873, 15982, 29892,   591,   674,   367,  9109,  1213,    13,\n          1576, 14280,  1833,   287,   363,   263,  1472,   931, 29892,   541,\n         10201,   372, 11084, 29889,   450,  6575,  2996,   714,   322,   372,\n           471,   263,  9560,  2462, 29889,    13, 29933,   433,   381,   471,\n           577,  9796,   304,  1074,   278,  6575,   845,   457,   322,  6350,\n           304,   341,   290,  1357, 29892,  1058,   471,  1560,  6504, 29889,\n          2296,  1497, 29892,   376, 12024, 29915, 29879,   748,  5377,   322,\n          1708,  3850,    13, 29933,   433,   381,   471,   577, 24173,   322,\n           896,  3512,  5377,   304,  1708, 29889,  2688,   750,   577,  1568,\n          2090, 29991,     1]], device='cuda:0')\n====================Decoded Results====================\nBlair is a cute little girl lived in a happy family. She was always playing with her toys and running around the house. One day, she was playing with her toy car when she heard a loud noise. She looked up and saw a big, black cloud in the sky.\nMommy said, \"We need to go to the doctor, ears!\"\nBlair was scared, but she knew that Mommy was right. They went to the doctor and he said that the cloud was a bad storm. He said they needed to stay inside until the storm was over.\nBlair was sad, but Mommy said, \"Don't worry, we will be safe.\"\nThe storm lasted for a long time, but eventually it stopped. The sun came out and it was a beautiful day.\nBlair was so happy to see the sunshine and ran to Mommy, who was smiling. She said, \"Let's go outside and play!\"\nBlair was so excited and they went outside to play. They had so much fun!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Skeleton Code: Single input (same as previous code blocks)\n\nprompt = \"Blair is a cute little girl lived in a happy family.\" # ⬅️ try to construct different prompts.\n\n# ⬇️ Try to tune different decoding hyperparameters.\n# You can also add more hyperparameters like `top_p`, `top_k`.\nmax_new_tokens = 500\ndo_sample = True\ntemperature = 0.5\ntop_p = 0.95\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=top_p,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"id":"DNvZ5Q6rYeC4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726499605043,"user_tz":-480,"elapsed":4842,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"86744250-4092-4084-ffef-7dc2324ce057","execution":{"iopub.status.busy":"2024-09-16T20:03:03.548395Z","iopub.execute_input":"2024-09-16T20:03:03.549244Z","iopub.status.idle":"2024-09-16T20:03:05.336383Z","shell.execute_reply.started":"2024-09-16T20:03:03.549203Z","shell.execute_reply":"2024-09-16T20:03:05.335463Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"<s> Blair is a cute little girl lived in a happy family. Every day, she would play with her toys and laugh with her family.\nOne day, her mommy asked her to help with the laundry. \"Bishies are not available right now,\" said mommy.\nBlair was sad and said, \"I don't want to help. I want to play.\"\nMommy said, \"I know you want to play, but we need to do the laundry first. It's important to help our family.\"\nBlair thought for a moment and then said, \"Okay, I'll help.\" She smiled and ran to get the laundry.\nMommy was very happy and said, \"Thank you, honey. You are such a good helper.\"\nBlair smiled and said, \"I'm glad I could help. Now, let's do the laundry together.\"<s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Skeleton Code: Bacthed input-output\n\nprompts = [\"Blair is a cute little girl lived in a happy family. \", \"She got a little puppy as a gift one day.\"]  # ⬅️ try to construct different prompts.\n\nbatch_size = 2 # If you have multiple data inputs, please control the batch size to prevent out-of-memory issues.\n\n# ⬇️ Try to tune different decoding hyperparameters.\n# You can also add more hyperparameters like `top_p`, `top_k`.\nmax_new_tokens = 500\ndo_sample = True\ntemperature = 0.8\ntop_p = 0.95\n\nfor i in range(0, len(prompts), batch_size):\n    batch_input = prompts[i:i+batch_size]\n    tokenized_input = tokenizer(batch_input, return_tensors=\"pt\", padding=True).to(device)\n\n    # For decoder-only models, batched inputs of model.generate() should be in the format of input_ids.\n    output_ids = model.generate(\n        tokenized_input[\"input_ids\"],\n        max_new_tokens=max_new_tokens,\n        eos_token_id=1,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n\n    for idx, result in enumerate(output_text):\n        print(f\"{result}\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80jDK-9uP1IO","outputId":"d3c9b390-4064-41db-ee06-13b478cd1e27","executionInfo":{"status":"ok","timestamp":1726499658570,"user_tz":-480,"elapsed":3926,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:03:11.121159Z","iopub.execute_input":"2024-09-16T20:03:11.122189Z","iopub.status.idle":"2024-09-16T20:03:13.587792Z","shell.execute_reply.started":"2024-09-16T20:03:11.122135Z","shell.execute_reply":"2024-09-16T20:03:13.586834Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Blair is a cute little girl lived in a happy family.  She loved to play in the sunshine and explore the garden. \nOne day, when her mom was busy in the kitchen, she decided to go outside and explore the garden. She was excited as she ran around the garden, looking for new things to discover. \nSuddenly, she heard a loud noise. It was coming from the garden. She followed the sound and saw a large, round, yellow thing in the middle of the garden. It looked like a ball of gas. She was curious and wanted to know what it was. \nShe asked her mom, \"What is that thing in the garden?\" \nHer mom smiled and said, \"That is gas. It comes from the gas machine. It can be dangerous for you to touch it.\" \nBlair was scared and ran back into the house. But then she thought of something else. She said, \"I can be a smart girl and learn how to be safe around gas.\" \nHer mom smiled and said, \"That is a great idea! Let's go back outside and learn about gas.\"\nSo, the little girl and her mom went outside and spent the day learning about gas and other things that can be dangerous.\n\nShe got a little puppy as a gift one day. was so excited and couldn't wait to play with him. He ran around the backyard, chasing the puppy. \nThe puppy was so excited, he had never seen so many puppies before. He wanted to take him in the house, but his mom said no. She was afraid he might get hurt, so she said he could play outside. \nHe ran off to the backyard, feeling embarrassed. He stopped and looked around, but the puppy was nowhere to be seen. He started to feel worried and scared. \nThen, he heard a voice. It was the puppy's mom. She said: \"I have been looking for my little pup. He must have been scared.\" She took him inside and gave him a big hug. \nThe little puppy was relieved and happy. He was glad his mom was there to help him.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### What about other languages?","metadata":{"id":"rhfIINEMSIjp"}},{"cell_type":"markdown","source":"Oops! This English language model cannot generate stories in other languages!\n\nWhy? Let us evaluate the perplexity of different languages in the next task.","metadata":{"id":"pYlvKScxkUqT"}},{"cell_type":"code","source":"prompt = \"两只小老虎过马路\"\n\n# Decoding hyperparameters\nmax_new_tokens = 500\ndo_sample = True\ntemperature = 0.5\ntop_p = 0.95\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=top_p,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0jb_ZqW_ixo","outputId":"a554309d-8855-44a2-dcb2-1bbbdae400fb","executionInfo":{"status":"ok","timestamp":1726499733796,"user_tz":-480,"elapsed":3310,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:03:18.834696Z","iopub.execute_input":"2024-09-16T20:03:18.835078Z","iopub.status.idle":"2024-09-16T20:03:21.147668Z","shell.execute_reply.started":"2024-09-16T20:03:18.835043Z","shell.execute_reply":"2024-09-16T20:03:21.146706Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"<s> 两只小老虎过马路 was a very adventurous boy. He loved to explore and find new things. One day, he decided to go on a big adventure. He packed his bag and set off.\nAs he was walking, he saw a big tree with a big, juicy apple at the top. He wanted to eat it, but it was too high for him to reach. Suddenly, a friendly bird flew down and offered to help him. The bird flew up and grabbed the apple with its beak. The boy was so happy and grateful that he thanked the bird and shared the apple with it.\nAs they were eating, the bird told the boy a story about a magical apple that could grant wishes. The boy was so excited and wished for a new toy. Suddenly, a toy appeared in front of him. The boy was amazed and thanked the bird again. But as he turned around to thank the bird, he saw that it had disappeared. The boy realized that the bird had given him a special gift that made him forget about his adventure. He went home with a smile on his face, grateful for the unexpected surprise.<s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 2: Perplexity Evaluation","metadata":{"id":"DYst_QWTkUqU"}},{"cell_type":"markdown","source":"#### Background\n\n---\n\nThe perplexity serves as a key metric for evaluating language models. It quantifies how well a model predicts a sample, with lower perplexity indicating better performance. For a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, the perplexity is defined mathematically as:\n\n$$\\text{Perplexity}(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i=1}^t \\log p_\\theta (x_i | x_{<i}) \\right)$$\n\nHere, $p_\\theta(x_i | x_{<i})$ represents the probability of a token $ x_i $ given its preceding tokens, and the formulation incorporates the average log probability across the sequence.\n\n---\n\n⚠️ Please make sure to **run the following cell first** to define the evaluation function.\n\n😄 **You do not need to check these complex details! Too hard for beginners!** However, if you are interested, you can compare the following code with the explanations above to better understand how to implement PPL evaluation using PyTorch.","metadata":{"id":"T6NxwPeaBzOB"}},{"cell_type":"code","source":"# The following code was adapted from the `evaluate` library. Licensed under the Apache License, Version 2.0 (the \"License\").\n# We modify them to avoid causing serious memory issues in the Colab environment.\n\ndef compute_ppl(\n        model, tokenizer, inputs, device, batch_size: int = 16, add_start_token: bool = True, max_length=None\n):\n\n    if device is not None:\n        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n        if device == \"gpu\":\n            device = \"cuda\"\n    else:\n        device = \"cuda\" \n\n    # if batch_size > 1 (which generally leads to padding being required), and\n    # if there is not an already assigned pad_token, assign an existing\n    # special token to also be the padding token\n    if tokenizer.pad_token is None and batch_size > 1:\n        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n        # check that the model already has at least one special token defined\n        assert (\n            len(existing_special_tokens) > 0\n        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n        # assign one of the special tokens to also be the pad token\n        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n\n    if add_start_token and max_length:\n        # leave room for <BOS> token to be added:\n        assert (\n            tokenizer.bos_token is not None\n        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n        max_tokenized_len = max_length - 1\n    else:\n        max_tokenized_len = max_length\n\n    encodings = tokenizer(\n        inputs,\n        add_special_tokens=False,\n        padding=True,\n        truncation=True if max_tokenized_len else False,\n        max_length=max_tokenized_len,\n        return_tensors=\"pt\",\n        return_attention_mask=True,\n    )\n\n    encoded_texts = encodings[\"input_ids\"]\n    attn_masks = encodings[\"attention_mask\"]\n\n    # check that each input is long enough:\n    if add_start_token:\n        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n    else:\n        assert torch.all(\n            torch.ge(attn_masks.sum(1), 2)\n        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n\n    ppls = []\n    loss_fct = CrossEntropyLoss(reduction=\"none\")\n\n    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n        end_index = min(start_index + batch_size, len(encoded_texts))\n        encoded_batch = encoded_texts[start_index:end_index].to(device)\n        attn_mask = attn_masks[start_index:end_index].to(device)\n\n        if add_start_token:\n            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n            attn_mask = torch.cat(\n                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n            )\n\n        labels = encoded_batch\n\n        with torch.no_grad():\n            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n\n            shift_logits = out_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n\n            perplexity_batch = torch.exp(\n                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n                / shift_attention_mask_batch.sum(1)\n            )\n\n            ppls += perplexity_batch.tolist()\n\n    del encoded_batch, attn_mask\n    if device == \"cuda\":\n        torch.cuda.empty_cache()\n\n    return {\"perplexities\": ppls, \"mean_perplexity\": sum(ppls)/float(len(ppls))}","metadata":{"id":"bEIdmBRsJaP9","executionInfo":{"status":"ok","timestamp":1726509450595,"user_tz":-480,"elapsed":436,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T09:13:04.590320Z","iopub.execute_input":"2024-09-17T09:13:04.590693Z","iopub.status.idle":"2024-09-17T09:13:04.607376Z","shell.execute_reply.started":"2024-09-17T09:13:04.590655Z","shell.execute_reply":"2024-09-17T09:13:04.606450Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"\n#### 💡Tutorials: compute_ppl() function.\n\n---\nMinimal example:\n\n```python\ntest_dataset = [\"Once upon a time,\"]\n\ncompute_ppl(\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    inputs=test_dataset,\n    batch_size = 16\n)\n```\n\nImportant parameters:\n- `inputs`: list of input text, each separate text snippet is one list entry.\n- `batch_size`: the batch size to run evaluations.\n\nReturns:\n- `perplexity`: `{\"perplexities\": [x.x, x.x, ...], \"mean_perplexity\": x.x}` dictionary containing the perplexity scores for the texts in the input list, as well as the mean perplexity. .\n\n\n---","metadata":{"id":"9TscJWpxB2XH"}},{"cell_type":"markdown","source":"#### Task 2 Playground\n\n---\n\n📚 Task 2: Evaluate the perplexity. Ensure that you evaluate both the English and Chinese test data we provided. You are encouraged to collect more diverse text data and discuss your findings regarding the language understanding capacity of the base model.\n\n\nNote: If you want to reuse the evaluation codes for JSONL data, please structure the content as follows:\n```json\n{\"text\": \"one data\"}\n{\"text\": \"two data.\"}\n...\n```\n**You may find that the PPL value for Chinese text is significantly higher than that for English text. This is evidence that the base model cannot generate a Chinese story at the end of the last task.**\n\n---","metadata":{"id":"2yDr5pIVCHCP"}},{"cell_type":"code","source":"# Skeleton Code: Evaluate the perplexity (PPL) on a list of raw text.\n\ntest_dataset = [\"Blair is a cute little girl lived in a happy family. \", \"She got a little puppy as a gift one day.\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["712b0ee96b4f4356880e786c9fffad34","2eb612360c0b408ba7a6d5dc1b3a00a1","cf21243fe22749228b065d6178393b22","2c9df83597da404a8924597e3f6d4a64","299188bca98f4649ac5e4ef1b1567926","d2a947b204174eaf82bc1116e5807a33","cf00fe9cb8a340a88c5b857b07315f5c","fe9ee33360c241619998212768421fac","f51833b3bae5443a9fa88542e70387ea","88e79be25d2c40c0b625f4a9aefe4e3b","61209074c40c49f7b6cce129ae4cc0e9"]},"id":"QSWMQtwDSdIm","outputId":"bf6f6dc6-99a0-4f90-d91f-f29adbe211d1","executionInfo":{"status":"ok","timestamp":1726509466138,"user_tz":-480,"elapsed":3973,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T04:33:09.433299Z","iopub.execute_input":"2024-09-17T04:33:09.433711Z","iopub.status.idle":"2024-09-17T04:33:10.515772Z","shell.execute_reply.started":"2024-09-17T04:33:09.433672Z","shell.execute_reply":"2024-09-17T04:33:10.514844Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23360a317f13472d815d2fead3f66e22"}},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 78.03\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = [\"两只老虎跑得快\",\"一直没有尾巴，一直没有眼睛\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["86bafea191e1474b86ac6dbf2280c1d5","9eb1b43043f24dd691d3aa776f519ccf","77e5135e6b5d4f2085608ffa8995b74c","5eef88c405ba47289272b568c46ab891","c9ed3151f50d46c684a84b9a2fd9d048","2ea9203ed5c744018d20d17ba5befbce","33705f67b2aa4feab6f86ba678281090","31d442e3ce4f4cd08594feb7685c44d1","ed7f2ca35e934f119de7d9ce2f568941","c2842c8e678f406997801558bd338de9","fc8cdf5e81af4851bc36dcee023dd7bb"]},"id":"gYFSzsA89TlI","executionInfo":{"status":"ok","timestamp":1726509469377,"user_tz":-480,"elapsed":569,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"d9ec1b67-08e7-4c90-f059-ebd4a7417a4d","execution":{"iopub.status.busy":"2024-09-17T04:33:13.915259Z","iopub.execute_input":"2024-09-17T04:33:13.915699Z","iopub.status.idle":"2024-09-17T04:33:13.957925Z","shell.execute_reply.started":"2024-09-17T04:33:13.915657Z","shell.execute_reply":"2024-09-17T04:33:13.956999Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb1cf3794684fba8341b66a27005bf4"}},"metadata":{}},{"name":"stdout","text":"Perplexity: 126800.88\n","output_type":"stream"}]},{"cell_type":"code","source":"# Skeleton Code: Evaluate the perplexity (PPL) on an external test set file (JSONL).\n\n# English test set.\ndata_file = EN_TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nmodel.to(\"cuda\")\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(English Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n# Chinese test set.\ndata_file = TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Chinese Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n\n# Try your own data file!","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["f0e45d33cbd84ec6b07ef8acd3fa5fb1","bc496fbacdef434db21c7a614728fd59","445ecf6ce5b244038938398d2f1cacdd","c854a3b7152f4a43b146fd80904cb9f3","bf2a63976bd6400ab3e541ed2a9d5802","e4e5d5c6d70044629be88faea927fde7","ca1c1009f026404391b0a62a6acba693","3cfec95523a042f7bc5536c0ea2109df","7efed9a03d2b4ab286b8c92948f2f77b","03afd4d04a324576b65a62a6a2f956d4","90c406ec888947c3af6d75c26a6322c6","8e111b9924f94ebb8f58567f49f46384","519340a6577a4edfa1b2ddfb2c1ede43","dea0a7b1db514d328b96cd2a09baa4d9","e8310516f531423b825b72540d7571fa","aace4271a55944b4a15fe97dc55c3895","83509ffa228743bc9826878186585558","d373fded8b8c41f19675ce1b1c5e1c99","6bd42fb6928e40c1a63b3633591fa41c","1db1fd04e720478e9415b228288aabc4","fb06702a23e940c684568ca56ec59223","9987486369064fb2b1259ec39956c9fb"]},"id":"4a_qx_9LLHYI","outputId":"c7de7a1f-a86b-49c5-c181-74c772e3b232","executionInfo":{"status":"ok","timestamp":1726509604116,"user_tz":-480,"elapsed":132062,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T04:33:16.821494Z","iopub.execute_input":"2024-09-17T04:33:16.822512Z","iopub.status.idle":"2024-09-17T04:34:29.423191Z","shell.execute_reply.started":"2024-09-17T04:33:16.822471Z","shell.execute_reply":"2024-09-17T04:34:29.422199Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f53de1a887da49de90950df2e4fc5711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82c8ad404544e909006d7af82591a46"}},"metadata":{}},{"name":"stdout","text":"(English Text) Test Perplexity: 4.14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9603c8987fd414395f2fbdfb10e0bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836d56a5f6ca491880ee595a04ec3318"}},"metadata":{}},{"name":"stdout","text":"(Chinese Text) Test Perplexity: 70030.40\n","output_type":"stream"}]},{"cell_type":"code","source":"# 🚨 Release gpu cache before training the model\n\nimport gc\ngc.collect() # Python thing\n# torch.cuda.empty_cache() # PyTorch thing\nwith torch.no_grad():\n    torch.cuda.empty_cache()","metadata":{"id":"MVELmXzJP7GS","executionInfo":{"status":"ok","timestamp":1726509609584,"user_tz":-480,"elapsed":415,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T04:35:08.639637Z","iopub.execute_input":"2024-09-17T04:35:08.640837Z","iopub.status.idle":"2024-09-17T04:35:08.962535Z","shell.execute_reply.started":"2024-09-17T04:35:08.640783Z","shell.execute_reply":"2024-09-17T04:35:08.961234Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Task 3: Continual Pre-training (in Chinese or in another language you are proficient in)\n\nCurrently, our base English LM is proficient in English but lacks the capability to generate or comprehend other languages (e.g., Chinese). The objective of this task is to enhance a base English LM by continually pre-training it with text in another language. This process aims to enable the model to understand and generate mini-story in another language.\n\nWe have provided 10,000 Chinese training samples. The training process for any language is the same. We have included useful resource links (in Assignment description PDF) to help you create additional data. If you encounter any issues in creating a dataset in another language, please do not hesitate to contact us.\n\nWe have implemented data preprocessing and the training pipeline, so you are not required to optimize these components. Instead, focus on tuning the training hyperparameters and observe the changes in model performance.\n\n\n---\n\n⚠️ Please **make sure to run the following cell first to pre-process data**.\n\n😄 You do not need to check the details of whole pipeline construction! Please pay attention to the hyper-parameters of `trainer`.","metadata":{"id":"_gilWue9kUqU"}},{"cell_type":"markdown","source":"#### Preprocess Data\nHere, we preprocess (tokenize and group) the text for the subsequent evaluation and pre-training phases.","metadata":{"id":"dzYCSeGVkUqT"}},{"cell_type":"markdown","source":"Load prepared Chinese dataset from Google drive (or local disk).","metadata":{"id":"HhQN9DpQkUqT"}},{"cell_type":"code","source":"chinese_dataset = load_dataset('json', data_files={'train': TRAIN_FILE, 'validation':VALIDATION_FILE, 'test': TEST_FILE})\nprint(chinese_dataset)\nprint(chinese_dataset[\"test\"][2][\"text\"])","metadata":{"id":"SiGEfoUMkUqT","outputId":"c359d99e-ef62-4601-eea1-8236aa965465","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726509616877,"user_tz":-480,"elapsed":532,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T04:35:12.920223Z","iopub.execute_input":"2024-09-17T04:35:12.921083Z","iopub.status.idle":"2024-09-17T04:35:13.241250Z","shell.execute_reply.started":"2024-09-17T04:35:12.921046Z","shell.execute_reply":"2024-09-17T04:35:13.240405Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd072e72a754fe08c1cf35c0aa14534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af6589066244b2fbdc88e7804f4de18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5379afc5dc86435eb9054a912a4eb9bf"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 10000\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1000\n    })\n})\n从前，有一个小女孩名叫莉莉。她喜欢和家人一起去度假。有一天，她的家人决定去海边旅行。莉莉非常兴奋，她跳起来又跳下去，像发了疯一样。\n\n当他们到达海滩时，他们搭起了遮阳伞和毯子。莉莉想立刻去游泳，但她的父母告诉她要等吃完午餐再说。莉莉感到很不耐烦，她说：“我现在就想去游泳！”她妈妈回答：“莉莉，我们需要先吃东西。游泳需要能量。”\n\n莉莉意识到妈妈说得对，于是耐心地等待午餐结束。她学会了有时候要克制自己的激动情绪，并听从父母的意见。从那天起，莉莉变得更善于倾听，也更加享受她的假期时光。\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We tokenize the raw text using Llama-2's tokenizer and group the tokenized text as inputs.","metadata":{"id":"uAcSghJgkUqT"}},{"cell_type":"code","source":"block_size = 512\n\ndef group_texts(examples):\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"id":"tUTCHIqEWJRL","executionInfo":{"status":"ok","timestamp":1726509620662,"user_tz":-480,"elapsed":415,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-17T04:35:22.559680Z","iopub.execute_input":"2024-09-17T04:35:22.560503Z","iopub.status.idle":"2024-09-17T04:35:22.567101Z","shell.execute_reply.started":"2024-09-17T04:35:22.560463Z","shell.execute_reply":"2024-09-17T04:35:22.566093Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenized_zh_datasets = chinese_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True, num_proc=4, remove_columns=[\"text\"])\nlm_datasets = tokenized_zh_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=512,\n    num_proc=4,\n)","metadata":{"id":"dXoXO29hWY-v","colab":{"base_uri":"https://localhost:8080/","height":300,"referenced_widgets":["cdec9f2f21db475ea081cf0e7a997728","f06ecbe32a054ea292bbf2b6ebffed76","cf656c7639104dddaa2b6f5069bf9c7a","f6a5c9bfbd7e4ef9b791e534842fdb5f","33876344f46f43459ad7df5b86371ff3","2d0e162e51ae4d8795b2c0c071bc6306","1cd27fceeabd4524b3030890d3f13529","2eb047db199449cbb749fc7fccf9b4ad","62cbb07ff7244c82b4ba3782f93a8274","2c396d4c975b4755b962ec31cdb68594","22ae5f8af4304f4c9f7cecd127a421a1","05e5b5db2043429e924e89855d0f0a84","df2a1a64dfa84edbb89adc00b77a135d","4bd4cb0467884fad8ea412d126948e7c","2bd39897984e4821bf39ed38b00c5443","298bb1e5e1064935867c0ae1a3ea064d","44b20dcf718f43e69f764ea1c6a3200d","a234735800eb43349e3de43e8dd85f95","7552e6e77dcf47e5b2809f9c8c8e6942","14d05dbaab7f4052a153b5e679528735","8a4995c046784061a30635df41bb7b95","56914d68eff7401cb65fe43a7293ecf0","7f98d4c9a0674a258161c7537a42fb13","312f1477445d4b6d862bf6875ad08651","f5b9506ab6104e09a6994a4c8efe4846","5478a67c333d4de89b522ce611d0e57a","aea9d7797d954a3586c480ae57f76596","59a52aa9700a41629b157fd49bd03ef7","2635c9860e8c44dfa775331db4a725b7","6666ec9bc9a544448004a47d714d2588","48e906f4b4af447ca55e45844ecf75c6","75e8b6e205934aeaa24d6b9a88f4ffa3","032a6aa2299342e9965da2369bbbf3be","55d343f194ea433b902a36f47666a297","4acc352f117840aa8dba9ddd4ccfc81f","eb0fd203f99b47398186f76d6528500a","c4f3f7290fac4d1f99b6fc2b24c2d5db","3f9320ec66b74fc1aa6bf704e05ab64c","253f0fee802e4e6aa0b4b06565728a36","ca08f1fb447e4e6884374fc41cc8d2a9","29ecbda152294fabb2a476e1be34da95","9f7787d08a1442338713956aabbbff3a","e80a4b8f0fe543a8a01b152c3741939f","e4aefa0a70314b799035fa2a2a0429fc","9e4d66441f0e4eb8848081bf2d12d468","16717142a2b94afdb9abd8902507fd82","e51707e7caa74784a39443cd04a8dffd","9ae46aefada348e0b6c8c3b6dc531e34","fa60d77d4a954bf687a54ae27ba80291","91f085759fe54812a24d0da509b19f41","68d94b51b75648458abd53a677675d3c","18d5f9e8627844df8d6f8695fcb9d8c2","dc68a8dc5d5846a184940c3d07d93073","c91d1855d2844cc180ea4e0d0630dea6","cd45a209d42e442c82473658434d835a","d4d11ea6f4a34f9ca469d4e0ca60a210","98dfd0b91af643b48f4a898d029a5147","328bec1a47824ec0b8b7e52f2ac873e5","e7de55aa81284c1994064b7e4390133f","f409deee3ecb4d9f96d1bdf1f17374e2","705a6c6cf2c34b18b8420df13de184ad","533639e655ba4af5babc4649b6066394","d0e1bfa217964f23a4ee14c17b271d6a","e2ae50c832a74456acc48749788db2dc","61acfd39c31d4724a064faf094a2f1d8","0079eb5dadbf4751a41b6b795e4041d9"]},"executionInfo":{"status":"ok","timestamp":1726509646989,"user_tz":-480,"elapsed":18293,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"71b454ac-4604-4812-9c4e-3a7689322436","execution":{"iopub.status.busy":"2024-09-17T04:35:27.376650Z","iopub.execute_input":"2024-09-17T04:35:27.377397Z","iopub.status.idle":"2024-09-17T04:35:35.366125Z","shell.execute_reply.started":"2024-09-17T04:35:27.377360Z","shell.execute_reply":"2024-09-17T04:35:35.365228Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723084863ca0424ba91405d0ca091692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e7d54805394e14b45101385b38bce6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e56cc48a7a4f03be7008d8a702bd75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b041d42a5c4630ab7ae628614da913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e4a769f3874ae8ac0d6d7d86e8aa9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167b9058c9244daa92533e83e503849a"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"0_m505-vnSBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 💡Tutorials: TrainingArguments().\n\n**Important Training Hyper-parameters**\n- learning_rate: The initial learning rate for optimizer.\n- num_train_epochs: Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).\n- *_strategy: The evaluation/saving strategy to adopt during training. Possible values are:\n    - `\"no\"`: No evaluation/saving is done during training.\n    - `\"steps\"`: Evaluation/saving is done (and logged) every `eval_steps`.\n    - `\"epoch\"`: Evaluation/saving is done at the end of each epoch.\n- per_device_train_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n- per_device_eval_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n- save_total_limit: If a value is passed, will limit the total amount of checkpoints.\n\n\n---\n\nIf you do not understand `AdamW` optimizer and learning scheduler, you may use default settings.\n\n**Optimizer Hyper-parameters**\n- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.\n- adam_beta1: The beta1 hyperparameter for the [`AdamW`] optimizer.\n- adam_beta2: The beta2 hyperparameter for the [`AdamW`] optimizer.\n\n**Learning schedule**\n- lr_scheduler: The scheduler type to use.\n- warmup_ratio: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n\n[Explore more parameters here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments)","metadata":{"id":"-DUTC5J7nSZt"}},{"cell_type":"markdown","source":"#### Task 3 Playground\n\n---\n\n📚 Please just run the following code to do continual pre-training. Please try your best to tune the hyperparameters or collect more data to improve model performance.\n\n---","metadata":{"id":"R9OJF3VNhrGK"}},{"cell_type":"code","source":"# =========Pre-training hyperparameters, please feel free to tune them~=========\n# =Important=\nlr = 1e-4\nepochs = 8\nsave_steps=200\nstrategy=\"steps\"\ntrain_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\neval_bsz = 16\n\n# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n# =Optimizer=\noptimizer = \"adamw_torch\"\nweight_decay = 0.01\nadam_beta1 = 0.9\nadam_beta2 = 0.98\n# =Learning scheduler=\nlr_scheduler = \"linear\"\nwarmup_ratio = 0.01\n# =========End of pre-training hyperparameters=========\n\n\ntraining_args = TrainingArguments(\n    \"llama-42m-zh-fairytales\",\n    evaluation_strategy = strategy,\n    eval_steps=save_steps,\n    save_strategy = strategy,\n    save_steps=save_steps,\n    logging_strategy=\"steps\",\n    logging_steps = 10,\n    learning_rate=lr,\n    weight_decay=weight_decay,\n    seed=42,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=eval_bsz,\n    save_total_limit=1,\n    optim = optimizer,\n    lr_scheduler_type = lr_scheduler,\n    adam_beta1 = adam_beta1,\n    adam_beta2 = adam_beta2,\n    warmup_ratio = warmup_ratio,\n    num_train_epochs = epochs,\n    report_to=None\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"id":"Lf6bS7lnbPMM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726505407665,"user_tz":-480,"elapsed":415,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"f9447c07-dea3-4cff-8000-3e752e00c8d0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"KCIR43fZcAa9","outputId":"8e287e54-925e-443e-d9d0-6fc4d82fa872","executionInfo":{"status":"ok","timestamp":1726508783118,"user_tz":-480,"elapsed":3368393,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}}},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2000 56:06, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>1.853600</td>\n","      <td>1.841430</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.504200</td>\n","      <td>1.522449</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.347700</td>\n","      <td>1.399509</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.238000</td>\n","      <td>1.333789</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.198100</td>\n","      <td>1.297980</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.149300</td>\n","      <td>1.266596</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.083200</td>\n","      <td>1.255093</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>1.033900</td>\n","      <td>1.247563</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.994500</td>\n","      <td>1.243147</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.008700</td>\n","      <td>1.238736</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":["TrainOutput(global_step=2000, training_loss=1.3321396446228027, metrics={'train_runtime': 3367.7817, 'train_samples_per_second': 18.93, 'train_steps_per_second': 0.594, 'total_flos': 4956004181606400.0, 'train_loss': 1.3321396446228027, 'epoch': 8.0})"]},"metadata":{}}]},{"cell_type":"markdown","source":"Load pre-trained model and try to generate mini-story in another language.","metadata":{"id":"TjA9aBL4kUqV"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device type: {device}\")\n\nnew_model_path = \"llama-42m-zh-fairytales/checkpoint-2000\" # saved checkpoint path\nmodel = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"lE_QVLCOkUqV","outputId":"f0353a2d-e88e-4ac0-d39e-b562a020cadb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726508784861,"user_tz":-480,"elapsed":506,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"Device type: cuda\n"}]},{"cell_type":"markdown","source":"Evaluate the PPL on Chinese text (or another language) again.\n\nYou will notice that we actually achieve a much lower PPL after continual pre-training.","metadata":{"id":"hlX2XilNkUqV"}},{"cell_type":"code","source":"data_file = TEST_FILE\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["492ce003eef64ac3ac80dac612549801","4ed14c9a805f4e7eb8246d2ea88306a0","14e490887b374de785ff3f197178d114","e199e46a27b746a19720c30a2b10b7cf","92333320977c4be2881265fa03b543bc","a5dc741290714eae8bc275fad3f010af","af593615025f4ab8a78e0f10001a4738","b9ee611c928f450db9430eecc0e494aa","35bb0c021a4f4a5ca347b5f3b844a079","d78c6d0322804c18bb09fb426d63b0ab","7ff172e076b740b6b8ed2ac118f8d6a2"]},"id":"0Ob_g328cGq5","outputId":"fa29516a-03e8-463a-b03e-68ce38414f4a","executionInfo":{"status":"ok","timestamp":1726508907910,"user_tz":-480,"elapsed":82264,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}}},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/63 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492ce003eef64ac3ac80dac612549801"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Test Perplexity: 3.37\n"}]},{"cell_type":"code","source":"test_dataset = [\"两只老虎跑得快\",\"一直没有尾巴，一直没有眼睛\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["708b7aa0216c4551b27de8db36029795","081295bb7a024a67848013e7138a6286","2684491d194e496eb756dafb2b01a362","b497260337d04c9e90f1860edc7bf8d5","dcba8d04bd5e4278b73bbbb6c63163c1","d667cdf69bbd4c18be8903c4b6aa5165","5652c3d566684fff922dc81133cc35bd","9fb3b0b8b4b14e7caa8e56504a71c733","8399c5c0f2a6407a9d5267e36eba17d3","84be9d87031c459e9faadf228c995b4a","3477d5f5f70f4aed85cdda7b2e851687"]},"id":"Eoeg-lbr8-sw","executionInfo":{"status":"ok","timestamp":1726509009532,"user_tz":-480,"elapsed":485,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"c0067953-8c1c-4694-ac1b-9a002973142f"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708b7aa0216c4551b27de8db36029795"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Perplexity: 13.89\n"}]},{"cell_type":"markdown","source":"---\n\nThe original English base model was pre-trained on 2 million data samples. Considering we are using only 10,000 training samples (0.5% of the original pre-training data), the model can generate a few fluent sentences but may still struggle with long-text generation or common sense of other languages. You can try using more data or training steps depending on your computational resources.\n\n---","metadata":{"id":"Xgi1EMDycZUp"}},{"cell_type":"code","source":"prompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"id":"Rf9ALF4CEX82","outputId":"28bec3be-6591-495c-e404-96ca3eb6e62b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726508915833,"user_tz":-480,"elapsed":3282,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。他们喜欢在公园里玩耍。有一天，汤姆看到一个大滑梯。他想滑下来，但是它太高了。\n\n\n\n汤姆的朋友，一只名叫马克斯的狗，来了，他说：“马克斯，你能帮我滑下来吗？”马克斯看了看汤姆，说：“好的，汤姆。我们一起滑下来吧。”\n\n\n\n汤姆和马克斯整天都在一起玩耍。他们滑下来，吃了蛋糕。马克斯很高兴他能帮助他的朋友。从那天起，汤姆和马克斯成为了最好的朋友。<s>\n"}]},{"cell_type":"code","source":"","metadata":{"id":"JJYZCXX3kUqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"改变超参数样例1","metadata":{"id":"oA1RkjiEubWt"}},{"cell_type":"code","source":"if device == \"cuda\":\n    torch.cuda.empty_cache()","metadata":{"id":"NJJC-biR9sAE","executionInfo":{"status":"ok","timestamp":1726509656604,"user_tz":-480,"elapsed":485,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"execution":{"iopub.status.busy":"2024-09-16T20:01:49.099519Z","iopub.execute_input":"2024-09-16T20:01:49.099935Z","iopub.status.idle":"2024-09-16T20:01:49.227721Z","shell.execute_reply.started":"2024-09-16T20:01:49.099897Z","shell.execute_reply":"2024-09-16T20:01:49.226471Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# =========Pre-training hyperparameters, please feel free to tune them~=========\n# =Important=\nlr = 1e-5\nepochs = 10\nsave_steps=200\nstrategy=\"epoch\"\ntrain_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\neval_bsz = 16\n\n# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n# =Optimizer=\noptimizer = \"adamw_torch\"\nweight_decay = 0.01\nadam_beta1 = 0.9\nadam_beta2 = 0.98\n# =Learning scheduler=\nlr_scheduler = \"linear\"\nwarmup_ratio = 0.01\n# =========End of pre-training hyperparameters=========\n\n\ntraining_args = TrainingArguments(\n    \"llama-42m-zh-fairytales\",\n    evaluation_strategy = strategy,\n    eval_steps=save_steps,\n    save_strategy = strategy,\n    save_steps=save_steps,\n    logging_strategy=\"steps\",\n    logging_steps = 10,\n    learning_rate=lr,\n    weight_decay=weight_decay,\n    seed=42,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=eval_bsz,\n    save_total_limit=1,\n    optim = optimizer,\n    lr_scheduler_type = lr_scheduler,\n    adam_beta1 = adam_beta1,\n    adam_beta2 = adam_beta2,\n    warmup_ratio = warmup_ratio,\n    num_train_epochs = epochs,\n    report_to=None\n)\n\ntrainer2 = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"id":"LjSLlPz7uaB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726514266528,"user_tz":-480,"elapsed":416,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"20b46ba8-046d-4365-cdc6-1ba5b6857de4"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"trainer2.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"2hAMvAecwNnb","outputId":"84d44e90-f6b0-47a4-9373-fc1932281d7a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='89' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  89/2500 02:21 < 1:05:25, 0.61 it/s, Epoch 0.35/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device type: {device}\")\n\nnew_model_path = \"llama-42m-zh-fairytales/checkpoint-2500\" # saved checkpoint path\nmodel2 = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\ntokenizer2 = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer2.pad_token is None:\n    tokenizer2.pad_token = tokenizer2.eos_token","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SueSohutwQlU","executionInfo":{"status":"ok","timestamp":1726514006189,"user_tz":-480,"elapsed":826,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"165b9649-ffb0-4f5c-e233-e0eebbae4733"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Device type: cuda\n"}]},{"cell_type":"code","source":"# model2\nprompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input2 = tokenizer2.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids2 = model2.generate(\n    tokenized_input2,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text2 = tokenizer2.decode(output_ids2[0])\nprint(output_text2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaiI-ZtpwYu5","executionInfo":{"status":"ok","timestamp":1726514019494,"user_tz":-480,"elapsed":6447,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"ba60d524-54e3-49bf-a4ff-9cc19276ef8e"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。她喜欢在一个叫莉莉。她喜欢在一个玩很很妈。她在一个玩具。\n\n\n\n有一天，莉莉的妈妈妈说：“妈妈，莉莉。我们把它。”\n\n\n\n莉莉和她的妈妈说：“妈妈妈，妈妈，我们的朋友们帮助你。”\n\n\n\n莉莉莉她的妈妈说：“我们怎么叫莉莉。我们很妈妈。”\n\n\n\n莉莉莉她的妈妈说：“妈妈，莉莉。我们可以很妈妈妈。”\n\n\n\n莉莉莉的妈妈说：“妈妈，����\n"}]},{"cell_type":"code","source":"# model2\ntest_dataset = [\"两只老虎跑得快\",\"一直没有尾巴，一直没有眼睛\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model2, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["5b48e11584414824a2312e3d8b1dce3b","00f0ad8bd05f4cc9a66d553ac8b98c18","77be0c835aaf487a89e9dc931e5cc1f4","175317bd9d3c4129a05415c1232cac4a","6d0144141c8e413da53755ada397a41f","db1e9b62c7de45949f6cc5a0a3b334c1","f31f487031a64cfe942b8f2ff689f094","4cb6052391854359ae6a1c3f91f86066","3af00981fb514fa6a32771c5f1d4dc4f","58f54cffb3f9474888bc4989a6ef00fe","b2db71f3802a4b7898a68f2f58c22b03"]},"id":"FUwS1DWbQdlb","executionInfo":{"status":"ok","timestamp":1726514095325,"user_tz":-480,"elapsed":433,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"3c01498d-15a2-4eb2-a178-9190092f334c"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b48e11584414824a2312e3d8b1dce3b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Perplexity: 39.42\n"}]},{"cell_type":"code","source":"# model2\ndata_file = TEST_FILE\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["5e212c8931aa49c8a80d78de82d398a9","4019aff066bb491eb1d9f1f96c3da498","3f7776a8bee74cc0bd7dfa19945e00c8","373b0a56e2dc41ba959060161e281476","3d82c2047ec0476cbdd54cb2350530d5","2a031e38448d4820ab4e2625b3362a03","18881968593e41b0a9ee34949828293a","fda8b88ea6974851a9fae71085ff4220","c58c25ccb346411986473c6bec04673f","17c2134c57704a8b8ec19abc9d94ed9d","d008032ba5374f8699251233fbf43826"]},"id":"lToeGldCQrBY","executionInfo":{"status":"ok","timestamp":1726514200779,"user_tz":-480,"elapsed":83263,"user":{"displayName":"Hana Xu","userId":"10610163411111303243"}},"outputId":"b063243d-f353-4717-aa47-c6a83772b0d7"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/63 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e212c8931aa49c8a80d78de82d398a9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Test Perplexity: 16.73\n"}]},{"cell_type":"code","source":"","metadata":{"id":"qCKGhrOpQSBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"调参 ******************3*******************","metadata":{}},{"cell_type":"code","source":"# =========Pre-training hyperparameters, please feel free to tune them~=========\n# =Important=\nlr = 5e-5\nepochs = 12\nsave_steps=200\nstrategy=\"epoch\"\ntrain_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\neval_bsz = 16\n\n# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n# =Optimizer=\noptimizer = \"adamw_torch\"\nweight_decay = 0.01\nadam_beta1 = 0.9\nadam_beta2 = 0.98\n# =Learning scheduler=\nlr_scheduler = \"linear\"\nwarmup_ratio = 0.01\n# =========End of pre-training hyperparameters=========\n\n\ntraining_args = TrainingArguments(\n    \"llama-42m-zh-fairytales\",\n    evaluation_strategy = strategy,\n    eval_steps=save_steps,\n    save_strategy = strategy,\n    save_steps=save_steps,\n    logging_strategy=\"steps\",\n    logging_steps = 10,\n    learning_rate=lr,\n    weight_decay=weight_decay,\n    seed=42,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=eval_bsz,\n    save_total_limit=1,\n    optim = optimizer,\n    lr_scheduler_type = lr_scheduler,\n    adam_beta1 = adam_beta1,\n    adam_beta2 = adam_beta2,\n    warmup_ratio = warmup_ratio,\n    num_train_epochs = epochs,\n     report_to=\"none\"\n)\n\ntrainer3 = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T04:35:59.050758Z","iopub.execute_input":"2024-09-17T04:35:59.051721Z","iopub.status.idle":"2024-09-17T04:35:59.109492Z","shell.execute_reply.started":"2024-09-17T04:35:59.051675Z","shell.execute_reply":"2024-09-17T04:35:59.108401Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer3.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T04:36:09.034474Z","iopub.execute_input":"2024-09-17T04:36:09.034849Z","iopub.status.idle":"2024-09-17T05:11:19.880714Z","shell.execute_reply.started":"2024-09-17T04:36:09.034814Z","shell.execute_reply":"2024-09-17T05:11:19.879417Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 35:08, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.614400</td>\n      <td>3.610271</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.223500</td>\n      <td>2.264548</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.820700</td>\n      <td>1.813517</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.603300</td>\n      <td>1.641949</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.495500</td>\n      <td>1.543603</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.412500</td>\n      <td>1.483981</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.375900</td>\n      <td>1.445553</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.362600</td>\n      <td>1.419579</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.310200</td>\n      <td>1.401846</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.273000</td>\n      <td>1.393842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2500, training_loss=1.9941860012054444, metrics={'train_runtime': 2109.7646, 'train_samples_per_second': 37.772, 'train_steps_per_second': 1.185, 'total_flos': 6195005227008000.0, 'train_loss': 1.9941860012054444, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device type: {device}\")\n\nnew_model_path = \"llama-42m-zh-fairytales/checkpoint-2500\" # saved checkpoint path\nmodel3 = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-09-17T05:16:14.383055Z","iopub.execute_input":"2024-09-17T05:16:14.384050Z","iopub.status.idle":"2024-09-17T05:16:14.651130Z","shell.execute_reply.started":"2024-09-17T05:16:14.384005Z","shell.execute_reply":"2024-09-17T05:16:14.650113Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# model3\nprompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids2 = model3.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids2[0])\nprint(output_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T05:16:17.291110Z","iopub.execute_input":"2024-09-17T05:16:17.292296Z","iopub.status.idle":"2024-09-17T05:16:20.262362Z","shell.execute_reply.started":"2024-09-17T05:16:17.292239Z","shell.execute_reply":"2024-09-17T05:16:20.261403Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。他们喜欢在树林里玩耍。有一天，他们在树林里散步时看到了一个很大的杯子。它很害怕，但它太大了。\n\n汤姆把杯子带到了杯子，但他很害怕。他不想吃掉它。他想帮助它。他说：“我们去找吧。”汤姆把杯子找到了一个漂亮的杯子。他说：“哇，我们可以找到它！”\n\n汤姆和他的朋友们很高兴。他们找到了一个愉快的地方。他们找到了一个拿着杯子。他们决定玩杯子。他们把杯子放在树上，然后�\n","output_type":"stream"}]},{"cell_type":"code","source":"# model3\ntest_dataset = [\"两只老虎跑得快\",\"一直没有尾巴，一直没有眼睛\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model3, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T05:16:31.142561Z","iopub.execute_input":"2024-09-17T05:16:31.143270Z","iopub.status.idle":"2024-09-17T05:16:31.501655Z","shell.execute_reply.started":"2024-09-17T05:16:31.143227Z","shell.execute_reply":"2024-09-17T05:16:31.500731Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36b4a6d3d4844e49050befddeadc27a"}},"metadata":{}},{"name":"stdout","text":"Perplexity: 20.75\n","output_type":"stream"}]},{"cell_type":"code","source":"# model3\ndata_file = TEST_FILE\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model3, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T05:16:34.409938Z","iopub.execute_input":"2024-09-17T05:16:34.410331Z","iopub.status.idle":"2024-09-17T05:17:19.368223Z","shell.execute_reply.started":"2024-09-17T05:16:34.410292Z","shell.execute_reply":"2024-09-17T05:17:19.367378Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2189fbfd8543d38cb525c489e70b53"}},"metadata":{}},{"name":"stdout","text":"Test Perplexity: 3.98\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 印尼语数据集 训练","metadata":{}},{"cell_type":"code","source":"IN_RAW_DATA = '/kaggle/input/indonisan-raw/Indonesian_raw.csv'","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:30:33.411028Z","iopub.execute_input":"2024-09-17T09:30:33.411397Z","iopub.status.idle":"2024-09-17T09:30:33.415572Z","shell.execute_reply.started":"2024-09-17T09:30:33.411364Z","shell.execute_reply":"2024-09-17T09:30:33.414718Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nDF_IN_RAW = pd.read_csv(IN_RAW_DATA)\n\ncolumn_name = 'targets'  \ncolumn_data = DF_IN_RAW[column_name]\n\nprint(column_data)\ncolumn_data.to_csv('IN_targets.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:33:39.693152Z","iopub.execute_input":"2024-09-17T09:33:39.693539Z","iopub.status.idle":"2024-09-17T09:33:39.737101Z","shell.execute_reply.started":"2024-09-17T09:33:39.693503Z","shell.execute_reply":"2024-09-17T09:33:39.736155Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0       Terjemahan atau padanan teks tersebut dalam Ba...\n1       Terjemahan atau padanan teks tersebut dalam Ba...\n2       Terjemahan atau padanan teks tersebut dalam Ba...\n3       Terjemahan atau padanan teks tersebut dalam Ba...\n4       Terjemahan atau padanan teks tersebut dalam Ba...\n                              ...                        \n1060    Terjemahan atau padanan teks tersebut dalam Ba...\n1061    Terjemahan atau padanan teks tersebut dalam Ba...\n1062    Terjemahan atau padanan teks tersebut dalam Ba...\n1063    Terjemahan atau padanan teks tersebut dalam Ba...\n1064    Terjemahan atau padanan teks tersebut dalam Ba...\nName: targets, Length: 1065, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean data\nIN_RAW = pd.read_csv('IN_targets.csv')\nIN_CLEANED = column_data.apply(lambda x: x.replace('Terjemahan atau padanan teks tersebut dalam Bahasa Indonesia adalah:\\n\\n', ''))\nIN_CLEANED","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:35:28.873963Z","iopub.execute_input":"2024-09-17T09:35:28.874360Z","iopub.status.idle":"2024-09-17T09:35:28.890501Z","shell.execute_reply.started":"2024-09-17T09:35:28.874317Z","shell.execute_reply":"2024-09-17T09:35:28.889653Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0       Dung Tak Tak Dung Tak! Dung Tat Tak Dung Tak! ...\n1       \"Bolehkah kami ikut?\" pinta Syam. \"Lain kali, ...\n2       Dada memandu bagal (anak kuda dan keledai) mer...\n3       Kaki kiri Kiki terjepit di sela-sela batu yang...\n4       Chacha meninggalkan Kiki dengan Ma yang begitu...\n                              ...                        \n1060    Pada hari Kamis, Manu berpiknik. \"Ibu, bagaima...\n1061    Hari Jumat cuaca mendung. \"Ibu, apakah hujan a...\n1062    Hari Sabtu, langit bergemuruh. Geluduk terdeng...\n1063    Akhirnya, hujan turun juga! \"Hujan! Hujan!\" te...\n1064    \"Manu,\" panggil Ibu seraya mengejarnya, \"Kamu ...\nName: targets, Length: 1065, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"IN_CLEANED.to_csv('IN_CLEANED.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:45:36.942372Z","iopub.execute_input":"2024-09-17T09:45:36.942747Z","iopub.status.idle":"2024-09-17T09:45:36.965406Z","shell.execute_reply.started":"2024-09-17T09:45:36.942713Z","shell.execute_reply":"2024-09-17T09:45:36.964550Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_ratio = 0.6\nval_ratio = 0.3\ntest_ratio = 0.1\n\ntrain_val_ratio = train_ratio / (train_ratio + val_ratio)\n\ntrain_val_df, test_df = train_test_split(IN_CLEANED, test_size=test_ratio, random_state=42)\n\ntrain_df, val_df = train_test_split(train_val_df, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:39:40.417843Z","iopub.execute_input":"2024-09-17T09:39:40.418234Z","iopub.status.idle":"2024-09-17T09:39:40.439193Z","shell.execute_reply.started":"2024-09-17T09:39:40.418200Z","shell.execute_reply":"2024-09-17T09:39:40.438432Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(train_df)\nprint(val_df)\nprint(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:50:52.295191Z","iopub.execute_input":"2024-09-17T09:50:52.295712Z","iopub.status.idle":"2024-09-17T09:50:52.305539Z","shell.execute_reply.started":"2024-09-17T09:50:52.295657Z","shell.execute_reply":"2024-09-17T09:50:52.304143Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"184    Mengenal Mohana 9 Mei 1971: Hari lahir Rajmoha...\n845    \"Nah, kalian berdua memiliki hal-hal membahagi...\n404    Pintu melihat bannya dan mengerti apa yang dis...\n901    Ada beberapa legenda tentang penemuan kopi. Sa...\n108    Manusia telah ada selama sekitar 200.000  tahu...\n                             ...                        \n72     Ranj tidak tahan. Dia merindukan semua teman-t...\n329    Ibu Ammu kemudian mengambil peniti dan menusuk...\n812    Kakek memakai tas yang sangat kecil, dan memba...\n618    Aku pikir mungkin wanita tua yang tinggal di l...\n55     Kita bisa dengan mudah mengetahui cuaca di lua...\nName: targets, Length: 638, dtype: object\n200     Suara itu terdengar lebih pelan sekarang. KECI...\n93      \"Jadi, aku mendapat gen ini dari Ibu?\" tanya V...\n481     Si adik menjawab, \"Ada banyak hewan ternak di ...\n729     Pak Tani meminta, \"Tolong turunkan hujan. Aku ...\n1055    Aku tidak keberatan , karena kami akan bermain...\n                              ...                        \n662     Suatu hari, ibu Cheeko masuk dengan muka merah...\n565     Begitu Tara, Madhav, Ibu, dan Ayah memasuki ru...\n819     Lalu ayah dan Hashim menjemur mangga-mangga it...\n398     Pak Ahmed menunjukkan kepada Pintu bagaimana s...\n13      Sepulang sekolah, Kiki dan saudara-saudaranya ...\nName: targets, Length: 320, dtype: object\n31      Ketika para ilmuwan pertama kali menemukan tul...\n832     Mama lalu berkata lagi, \"Ingatlah untuk tetap ...\n413     Sepanjang  tahun  berikutnya  tiap  pergantian...\n1047              Ketika aku menangis, dia juga menangis.\n874     Haruskah aku membeli buku yang banyak gambarny...\n                              ...                        \n538     Wow! Hebat sekali, Nettikutti! Sekarang kami p...\n109     Ini semua bisa terjadi karena semakin meningka...\n141     \"Kita punya buku-buku di rumah!\" kata kakaknya...\n231     Astronot Neil Armstrong menjadi manusia pertam...\n168     Di kebanyakan tempat di India, informasi masih...\nName: targets, Length: 107, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"df = train_df.reset_index(drop=True).to_frame()\ndf.columns = ['text']\n\n# 保存为 JSON Lines 格式\ndf.to_json('in_train.jsonl', orient='records', lines=True, force_ascii=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:07:20.437904Z","iopub.execute_input":"2024-09-17T10:07:20.438786Z","iopub.status.idle":"2024-09-17T10:07:20.447497Z","shell.execute_reply.started":"2024-09-17T10:07:20.438746Z","shell.execute_reply":"2024-09-17T10:07:20.446514Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df = val_df.reset_index(drop=True).to_frame()\ndf.columns = ['text']\ndf.to_json('in_val.jsonl', orient='records', lines=True, force_ascii=False)\n\ndf = test_df.reset_index(drop=True).to_frame()\ndf.columns = ['text']\ndf.to_json('in_test.jsonl', orient='records', lines=True, force_ascii=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:08:14.036964Z","iopub.execute_input":"2024-09-17T10:08:14.037850Z","iopub.status.idle":"2024-09-17T10:08:14.046913Z","shell.execute_reply.started":"2024-09-17T10:08:14.037811Z","shell.execute_reply":"2024-09-17T10:08:14.045995Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indonisia_dataset = load_dataset('json', data_files={'train': '/kaggle/working/in_train.jsonl', 'validation':'/kaggle/working/in_val.jsonl', 'test': '/kaggle/working/in_test.jsonl'})\nprint(indonisia_dataset)\nprint(indonisia_dataset[\"test\"][2][\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:08:19.464711Z","iopub.execute_input":"2024-09-17T10:08:19.465466Z","iopub.status.idle":"2024-09-17T10:08:19.785174Z","shell.execute_reply.started":"2024-09-17T10:08:19.465424Z","shell.execute_reply":"2024-09-17T10:08:19.784232Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9d0f994e6846da94ac336d24cb6e0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5423edeb67f44e1a19ae71c4fbdac97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eae855e9ab343c0acc0c89cb5a7920e"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 638\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 320\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 107\n    })\n})\nSepanjang  tahun  berikutnya  tiap  pergantian  musim,  aku mengamati perubahan yang sangat luar biasa di kolam itu. Pada musim panas, kolam itu seperti lahan tandus yang kering bahkan tak bernyawa. Tetapi, begitu hujan turun, kehidupan seketika mulai kembali. Seperti orkestra yang menunggu aba-aba dari konduktor untuk mulai beraksi. Rintik hujan pertama yang jatuh ke tanah kering menimbulkan aroma alami. Bahkan dia juga melakukan apa yang pesulap tidak bisa wujudkan, mengubah tanah coklat menjadi oasis indah berwarna hijau dan biru.\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 512\n\ndef group_texts(examples):\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:10:53.145991Z","iopub.execute_input":"2024-09-17T10:10:53.146376Z","iopub.status.idle":"2024-09-17T10:10:53.153007Z","shell.execute_reply.started":"2024-09-17T10:10:53.146342Z","shell.execute_reply":"2024-09-17T10:10:53.152145Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"tokenized_in_datasets = indonisia_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True, num_proc=4, remove_columns=[\"text\"])\nlm_datasets = tokenized_in_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=512,\n    num_proc=4,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:11:19.146893Z","iopub.execute_input":"2024-09-17T10:11:19.147607Z","iopub.status.idle":"2024-09-17T10:11:21.464336Z","shell.execute_reply.started":"2024-09-17T10:11:19.147569Z","shell.execute_reply":"2024-09-17T10:11:21.463283Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/638 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39ba48a7ce84192946aed841ec552bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a51a9d390d7644de84618c12879f3fd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129c443c152347f3b4032620e54eb46e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/638 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cec9adefcb47a696bb940d3ce0ed2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1b491760d440feaf1623b6cb376c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04e7f5ead304b54a7ae8cc29a88abdf"}},"metadata":{}}]},{"cell_type":"markdown","source":"训练模型","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device type: {device}\")\n\nmodel_path = MODEL_FOLDER\n# Load model from local files\nmodel = LlamaForCausalLM.from_pretrained(model_path).to(device)\n# Load tokenizer from local files\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:22:45.507051Z","iopub.execute_input":"2024-09-17T10:22:45.507958Z","iopub.status.idle":"2024-09-17T10:22:45.754310Z","shell.execute_reply.started":"2024-09-17T10:22:45.507915Z","shell.execute_reply":"2024-09-17T10:22:45.753282Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = [\"Sepanjang  tahun  berikutnya  tiap  pergantian  musim\",\"Seperti orkestra yang menunggu aba-aba dari konduktor untuk mulai beraksi\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:22:46.268157Z","iopub.execute_input":"2024-09-17T10:22:46.269009Z","iopub.status.idle":"2024-09-17T10:22:46.497230Z","shell.execute_reply.started":"2024-09-17T10:22:46.268968Z","shell.execute_reply":"2024-09-17T10:22:46.496299Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f2ba3dd93a4633818619bff71fa655"}},"metadata":{}},{"name":"stdout","text":"Perplexity: 146538.51\n","output_type":"stream"}]},{"cell_type":"code","source":"# Indonesian test set.\ndata_file = '/kaggle/working/in_test.jsonl'\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nmodel.to(\"cuda\")\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Indonisian Text) Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:22:49.081625Z","iopub.execute_input":"2024-09-17T10:22:49.082565Z","iopub.status.idle":"2024-09-17T10:22:50.614954Z","shell.execute_reply.started":"2024-09-17T10:22:49.082521Z","shell.execute_reply":"2024-09-17T10:22:50.614046Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce395a181bc41fbb71493b51cfa003e"}},"metadata":{}},{"name":"stdout","text":"(Indonisian Text) Test Perplexity: 34980.24\n","output_type":"stream"}]},{"cell_type":"code","source":"# =========Pre-training hyperparameters, please feel free to tune them~=========\n# =Important=\nlr = 5e-5\nepochs = 30\nsave_steps= 20\nstrategy=\"steps\"\ntrain_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\neval_bsz = 16\n\n# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n# =Optimizer=\noptimizer = \"adamw_torch\"\nweight_decay = 0.01\nadam_beta1 = 0.9\nadam_beta2 = 0.98\n# =Learning scheduler=\nlr_scheduler = \"linear\"\nwarmup_ratio = 0.01\n# =========End of pre-training hyperparameters=========\n\n\ntraining_args = TrainingArguments(\n    \"llama-42m-in-fairytales\",\n    evaluation_strategy = strategy,\n    eval_steps=save_steps,\n    save_strategy = strategy,\n    save_steps=save_steps,\n    logging_strategy=\"steps\",\n    logging_steps = 10,\n    learning_rate=lr,\n    weight_decay=weight_decay,\n    seed=42,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=eval_bsz,\n    save_total_limit=1,\n    optim = optimizer,\n    lr_scheduler_type = lr_scheduler,\n    adam_beta1 = adam_beta1,\n    adam_beta2 = adam_beta2,\n    warmup_ratio = warmup_ratio,\n    num_train_epochs = epochs,\n     report_to=\"none\"\n)\n\ntrainer_IN = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:22:52.654711Z","iopub.execute_input":"2024-09-17T10:22:52.655510Z","iopub.status.idle":"2024-09-17T10:22:52.693283Z","shell.execute_reply.started":"2024-09-17T10:22:52.655469Z","shell.execute_reply":"2024-09-17T10:22:52.692392Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_IN.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:22:55.206702Z","iopub.execute_input":"2024-09-17T10:22:55.207462Z","iopub.status.idle":"2024-09-17T10:25:23.020291Z","shell.execute_reply.started":"2024-09-17T10:22:55.207419Z","shell.execute_reply":"2024-09-17T10:25:23.019269Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 02:26, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>6.319200</td>\n      <td>5.844800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.927100</td>\n      <td>4.840263</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.185400</td>\n      <td>4.381457</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.759900</td>\n      <td>4.187562</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.426500</td>\n      <td>4.094593</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.179100</td>\n      <td>4.071829</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.962800</td>\n      <td>4.083347</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.812800</td>\n      <td>4.102041</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.748100</td>\n      <td>4.112710</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=180, training_loss=4.007002978854709, metrics={'train_runtime': 147.3581, 'train_samples_per_second': 34.406, 'train_steps_per_second': 1.222, 'total_flos': 394135732224000.0, 'train_loss': 4.007002978854709, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device type: {device}\")\n\nnew_model_path_in = \"llama-42m-in-fairytales/checkpoint-180\" # saved checkpoint path\nmodel_in = LlamaForCausalLM.from_pretrained(new_model_path_in).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:26:33.763060Z","iopub.execute_input":"2024-09-17T10:26:33.763968Z","iopub.status.idle":"2024-09-17T10:26:34.003562Z","shell.execute_reply.started":"2024-09-17T10:26:33.763927Z","shell.execute_reply":"2024-09-17T10:26:34.002499Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_in\nprompt = \"Dahulu kala, ada seekor kucing bernama Tom。\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids2 = model_in.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids2[0])\nprint(output_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:26:42.205715Z","iopub.execute_input":"2024-09-17T10:26:42.206615Z","iopub.status.idle":"2024-09-17T10:26:42.910008Z","shell.execute_reply.started":"2024-09-17T10:26:42.206568Z","shell.execute_reply":"2024-09-17T10:26:42.908999Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"<s> Dahulu kala, ada seekor kucing bernama Tom。 alam. Mereka mengikatnya keluar keluar dari kelu. Mereka menghampungnya menghormkannya di antar kelu. Mereka menghantungnya tidak menghanda dibakan kelu.<s>\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = [\"Sepanjang  tahun  berikutnya  tiap  pergantian  musim\",\"Seperti orkestra yang menunggu aba-aba dari konduktor untuk mulai beraksi\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model_in, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:26:47.304500Z","iopub.execute_input":"2024-09-17T10:26:47.304923Z","iopub.status.idle":"2024-09-17T10:26:47.628937Z","shell.execute_reply.started":"2024-09-17T10:26:47.304883Z","shell.execute_reply":"2024-09-17T10:26:47.627913Z"},"trusted":true},"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c14f96b31449118451fbba442c5fe0"}},"metadata":{}},{"name":"stdout","text":"Perplexity: 289.63\n","output_type":"stream"}]},{"cell_type":"code","source":"# Indonesian test set.\ndata_file = '/kaggle/working/in_test.jsonl'\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nmodel.to(\"cuda\")\nresults = compute_ppl(model=model_in, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Indonisian Text) Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:26:50.521071Z","iopub.execute_input":"2024-09-17T10:26:50.521990Z","iopub.status.idle":"2024-09-17T10:26:52.069571Z","shell.execute_reply.started":"2024-09-17T10:26:50.521950Z","shell.execute_reply":"2024-09-17T10:26:52.068649Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0ab3f4973c43df95db748c197e56a5"}},"metadata":{}},{"name":"stdout","text":"(Indonisian Text) Test Perplexity: 85.05\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
